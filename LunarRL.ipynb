{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduccion al problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium) (1.26.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium) (4.13.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows:\n",
    "\n",
    "method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install swig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[box2d] in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[box2d]) (1.26.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[box2d]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[box2d]) (4.13.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
      "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[box2d]) (2.6.1)\n",
      "Requirement already satisfied: swig==4.* in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[box2d]) (4.3.1)\n",
      "Building wheels for collected packages: box2d-py\n",
      "  Building wheel for box2d-py (setup.py): started\n",
      "  Building wheel for box2d-py (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for box2d-py\n",
      "Failed to build box2d-py\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [41 lines of output]\n",
      "      Using setuptools (version 80.2.0).\n",
      "      c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\setuptools\\_distutils\\dist.py:289: UserWarning: Unknown distribution option: 'test_suite'\n",
      "        warnings.warn(msg)\n",
      "      c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\setuptools\\dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "      \n",
      "              License :: OSI Approved :: zlib/libpng License\n",
      "      \n",
      "              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        self._finalize_license_expression()\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\\lib.win-amd64-cpython-312\\Box2D\n",
      "      copying library\\Box2D\\Box2D.py -> build\\lib.win-amd64-cpython-312\\Box2D\n",
      "      copying library\\Box2D\\__init__.py -> build\\lib.win-amd64-cpython-312\\Box2D\n",
      "      creating build\\lib.win-amd64-cpython-312\\Box2D\\b2\n",
      "      copying library\\Box2D\\b2\\__init__.py -> build\\lib.win-amd64-cpython-312\\Box2D\\b2\n",
      "      running build_ext\n",
      "      building 'Box2D._Box2D' extension\n",
      "      swigging Box2D\\Box2D.i to Box2D\\Box2D_wrap.cpp\n",
      "      swig.exe -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\\Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\\Box2D_wrap.cpp Box2D\\Box2D.i\n",
      "      Box2D\\Common\\b2Math.h(67) : Warning 302: Redefinition of identifier 'b2Vec2' by %extend ignored,\n",
      "      Box2D\\Box2D_math.i(47) : Warning 302: %extend definition of 'b2Vec2'.\n",
      "      Box2D\\Common\\b2Math.h(158) : Warning 302: Redefinition of identifier 'b2Vec3' by %extend ignored,\n",
      "      Box2D\\Box2D_math.i(168) : Warning 302: %extend definition of 'b2Vec3'.\n",
      "      Box2D\\Common\\b2Math.h(197) : Warning 302: Redefinition of identifier 'b2Mat22' by %extend ignored,\n",
      "      Box2D\\Box2D_math.i(301) : Warning 302: %extend definition of 'b2Mat22'.\n",
      "      Box2D\\Common\\b2Math.h(271) : Warning 302: Redefinition of identifier 'b2Mat33' by %extend ignored,\n",
      "      Box2D\\Box2D_math.i(372) : Warning 302: %extend definition of 'b2Mat33'.\n",
      "      Box2D\\Collision\\b2DynamicTree.h(44) : Warning 312: Nested union not currently supported (ignored).\n",
      "      Box2D\\Common\\b2Settings.h(144) : Warning 506: Can't wrap varargs with keyword arguments enabled\n",
      "      Box2D\\Common\\b2Math.h(91) : Warning 509: Overloaded method b2Vec2::operator ()(int32) effectively ignored,\n",
      "      Box2D\\Common\\b2Math.h(85) : Warning 509: as it is shadowed by b2Vec2::operator ()(int32) const.\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for box2d-py\n",
      "ERROR: Could not build wheels for box2d-py, which is required to install pyproject.toml-based projects\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install swig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ufal.pybox2d in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.10.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install ufal.pybox2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygame in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linux:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gymnasium.farama.org/environments/box2d/lunar_lander/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lunar import LunarLanderEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow or Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (80.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.27.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (80.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (1.26.1)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
      "Requirement already satisfied: optree in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gymnasium.spaces.box.Box'>\n",
      "<class 'gymnasium.spaces.discrete.Discrete'>\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "lunar = LunarLanderEnv()\n",
    "print(type(lunar.env.observation_space))\n",
    "print(type(lunar.env.action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El espacio de acciones es un valor del 0 al 3 que indica que acciones tomará el modulo lunar para esa iteración.\n",
    "\n",
    "en concreto son las siguientes:\n",
    "\n",
    "|value| action                        |\n",
    "|-----|-------------------------------|\n",
    "| 0   | do nothing                    |\n",
    "| 1   | fire left orientation engine  |\n",
    "| 2   | fire main engine              |\n",
    "| 3   | fire right orientation engine |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lunar.env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El espacio de observaciones son un conjunto de valores flotantes y booleanos que indica el estado del modulo lunar.\n",
    "\n",
    "en concreto son las siguientes:\n",
    "\n",
    "|value| observation                               |\n",
    "|-----|-------------------------------------------|\n",
    "| 0   | coordenada X (float)                      |\n",
    "| 1   | coordenada Y (float)                      |\n",
    "| 2   | velocidad lineal X (float)                |\n",
    "| 3   | velocidad lineal Y (float)                |\n",
    "| 4   | Angulo en radianes desde -2π a +2π (float)|\n",
    "| 5   | Velocidad angula (float)                  |\n",
    "| 6   | Contacto de la pierna Izquierda (bool)    |\n",
    "| 7   | Contacto de la pierna Derecha (bool)      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
       "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
       "  1.         1.       ], (8,), float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# se muestran los valores minimos y maximos del espacio de observaciones.\n",
    "lunar.env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observations: 8, actions: 4\n"
     ]
    }
   ],
   "source": [
    "observation_count = lunar.env.observation_space.shape[0] \n",
    "action_count = lunar.env.action_space.n\n",
    "\n",
    "print(f\"observations: {observation_count}, actions: {action_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
      "  -0.         -0.       ]\n",
      "[ 2.5        2.5       10.        10.         6.2831855 10.\n",
      "  1.         1.       ]\n"
     ]
    }
   ],
   "source": [
    "#valores minimos y maximos para las observaciones.\n",
    "print(lunar.env.observation_space.low) \n",
    "print(lunar.env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample ofrece una combinacion aleatoria del conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(lunar.env.action_space.sample())  # Take a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5636797   1.9987218   2.985093   -7.8622875  -5.434158   -5.457798\n",
      "  0.32740963  0.271745  ]\n"
     ]
    }
   ],
   "source": [
    "print(lunar.env.observation_space.sample())  # Sample a random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a random episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lunar_lander(steps_to_run_before_pause, agent, episodes=1):\n",
    "    \"\"\"\n",
    "    Test the Lunar Lander environment with a given agent.\n",
    "    \n",
    "    Parameters:\n",
    "    steps_to_run_before_pause (int): Number of steps to run before pausing for user input.\n",
    "    agent: The agent to be tested in the environment.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Initialize the environment\n",
    "    lunar = LunarLanderEnv(render_mode=\"human\")\n",
    "    \n",
    "    if(agent is not None):\n",
    "        # Set the agent's environment\n",
    "        agent.lunar = lunar\n",
    "        \n",
    "    for _ in range(episodes):\n",
    "        counter, score = 0, 0\n",
    "\n",
    "        while True:\n",
    "            if steps_to_run_before_pause != 0 and counter % steps_to_run_before_pause == 0:\n",
    "                input(\"Press Enter to continue...\")\n",
    "\n",
    "            if(agent is not None):\n",
    "                _, reward, done, action = agent.act()\n",
    "                \n",
    "            else:\n",
    "                # Sample a random action from the action space\n",
    "                action = lunar.env.action_space.sample()\n",
    "            \n",
    "                # Take a step in the environment\n",
    "                _, reward, done = lunar.take_action(action, verbose=True)\n",
    "                \n",
    "            score += reward\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            if done:\n",
    "                print(f\"Episode finished, score: {score}\")\n",
    "                break\n",
    "        if(agent is not None):\n",
    "            # Reset the agent's environment for the next episode\n",
    "            agent.lunar.reset()\n",
    "        else:\n",
    "            # Reset the environment for the next episode\n",
    "            lunar.reset()\n",
    "        \n",
    "    # Close the environment\n",
    "    lunar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step taken: 2, New state: [ 0.01537714  1.4177382   0.7855222   0.15480155 -0.0165947  -0.15589386\n",
      "  0.          0.        ], Reward: -3.098113660300629, Done: False\n",
      "Step taken: 2, New state: [ 0.02323895  1.421418    0.7936816   0.16344804 -0.02401757 -0.14847113\n",
      "  0.          0.        ], Reward: -2.3915933377075644, Done: False\n",
      "Step taken: 3, New state: [ 0.03117285  1.4244974   0.8027177   0.13668115 -0.03324736 -0.1846132\n",
      "  0.          0.        ], Reward: -1.6694467364490595, Done: False\n",
      "Step taken: 2, New state: [ 0.03907509  1.4281503   0.7998301   0.16211082 -0.04275134 -0.19009724\n",
      "  0.          0.        ], Reward: -1.8172408433737985, Done: False\n",
      "Step taken: 1, New state: [ 0.04688492  1.431212    0.78823197  0.13585258 -0.04991817 -0.14334966\n",
      "  0.          0.        ], Reward: 0.547783684581135, Done: False\n",
      "Step taken: 0, New state: [ 0.05469503  1.4336745   0.78825176  0.10918489 -0.05708499 -0.14335005\n",
      "  0.          0.        ], Reward: -0.5828708344874372, Done: False\n",
      "Step taken: 3, New state: [ 0.06256752  1.4355218   0.79607546  0.08175095 -0.06582873 -0.17489083\n",
      "  0.          0.        ], Reward: -1.5695411228124374, Done: False\n",
      "Step taken: 2, New state: [ 0.070397    1.4379535   0.7922169   0.10764114 -0.07502245 -0.1838918\n",
      "  0.          0.        ], Reward: -1.4218735066412933, Done: False\n",
      "Step taken: 3, New state: [ 0.07832336  1.4397798   0.80436134  0.08055029 -0.08664865 -0.23254493\n",
      "  0.          0.        ], Reward: -2.3047479685483254, Done: False\n",
      "Step taken: 2, New state: [ 0.08645668  1.4423373   0.8244184   0.11298185 -0.09765968 -0.22024086\n",
      "  0.          0.        ], Reward: -4.0768248368220155, Done: False\n",
      "Step taken: 1, New state: [ 0.0945137   1.4443024   0.8148352   0.08672729 -0.10673633 -0.18154903\n",
      "  0.          0.        ], Reward: 0.08444452240999681, Done: False\n",
      "Step taken: 2, New state: [ 0.10252304  1.446262    0.81047964  0.0863758  -0.11623669 -0.19002433\n",
      "  0.          0.        ], Reward: -1.0631762549922315, Done: False\n",
      "Step taken: 0, New state: [ 0.11053276  1.4476223   0.81050634  0.05969409 -0.12573475 -0.18997839\n",
      "  0.          0.        ], Reward: -0.9075038493950558, Done: False\n",
      "Step taken: 1, New state: [ 0.11846485  1.4483874   0.80078393  0.03335623 -0.13327697 -0.15085849\n",
      "  0.          0.        ], Reward: 0.19929955200660857, Done: False\n",
      "Step taken: 1, New state: [ 0.12631759  1.4485601   0.79082704  0.00716464 -0.13881287 -0.11072788\n",
      "  0.          0.        ], Reward: 0.3949725163912785, Done: False\n",
      "Step taken: 3, New state: [ 0.13424215  1.4481077   0.7998595  -0.02080436 -0.14620616 -0.14787905\n",
      "  0.          0.        ], Reward: -1.7223284489028845, Done: False\n",
      "Step taken: 2, New state: [ 0.1424366   1.4481971   0.82610095  0.00331511 -0.15286568 -0.13320272\n",
      "  0.          0.        ], Reward: -3.6505308565605388, Done: False\n",
      "Step taken: 0, New state: [ 0.15063123  1.4476871   0.82611674 -0.02336816 -0.15952517 -0.13320191\n",
      "  0.          0.        ], Reward: -0.7316818821113884, Done: False\n",
      "Step taken: 2, New state: [ 0.15908079  1.4479516   0.8511032   0.01108488 -0.16568743 -0.12325579\n",
      "  0.          0.        ], Reward: -3.5051934985944397, Done: False\n",
      "Step taken: 2, New state: [ 0.16766778  1.4488358   0.86484337  0.03860816 -0.17184176 -0.12309784\n",
      "  0.          0.        ], Reward: -2.552488662051462, Done: False\n",
      "Step taken: 0, New state: [ 0.17625514  1.4491204   0.8648589   0.01193027 -0.17799495 -0.12307476\n",
      "  0.          0.        ], Reward: -0.6684250213405676, Done: False\n",
      "Step taken: 0, New state: [ 0.18484268  1.4488056   0.8648764  -0.01473888 -0.18414724 -0.12305697\n",
      "  0.          0.        ], Reward: -0.6962438046535908, Done: False\n",
      "Step taken: 2, New state: [ 0.19369526  1.4486204   0.89058363 -0.00889232 -0.18950249 -0.10711505\n",
      "  0.          0.        ], Reward: -3.4944571625842, Done: False\n",
      "Step taken: 1, New state: [ 0.20245428  1.4478579   0.8788412  -0.03427467 -0.19245304 -0.05901608\n",
      "  0.          0.        ], Reward: 0.7436802762221089, Done: False\n",
      "Step taken: 2, New state: [ 0.2115076   1.4479486   0.9081603   0.00366076 -0.19529776 -0.05689456\n",
      "  0.          0.        ], Reward: -3.58740389854255, Done: False\n",
      "Step taken: 0, New state: [ 0.22056599  1.447427    0.90825844 -0.02350754 -0.1977384  -0.04881237\n",
      "  0.          0.        ], Reward: -0.36566573265463376, Done: False\n",
      "Step taken: 1, New state: [ 0.22954197  1.4463255   0.89792645 -0.04899978 -0.19806316 -0.00649534\n",
      "  0.          0.        ], Reward: 0.8384268095944958, Done: False\n",
      "Step taken: 2, New state: [ 0.2385374   1.4459081   0.90054035 -0.01868492 -0.19907938 -0.020324\n",
      "  0.          0.        ], Reward: -0.6513055999259223, Done: False\n",
      "Step taken: 2, New state: [ 0.2477726   1.4457963   0.92400324 -0.0050381  -0.19956025 -0.00961755\n",
      "  0.          0.        ], Reward: -2.8184945423598036, Done: False\n",
      "Step taken: 2, New state: [ 0.25728148  1.4459164   0.9506299   0.00536994 -0.19931023  0.00500051\n",
      "  0.          0.        ], Reward: -3.1132467051425463, Done: False\n",
      "Step taken: 3, New state: [ 0.26686564  1.4454172   0.96008855 -0.02240873 -0.20100002 -0.03379557\n",
      "  0.          0.        ], Reward: -1.2913060873214863, Done: False\n",
      "Step taken: 1, New state: [ 2.7638406e-01  1.4443414e+00  9.5182496e-01 -4.7810614e-02\n",
      " -2.0097278e-01  5.4481474e-04  0.0000000e+00  0.0000000e+00], Reward: 0.6350936867685004, Done: False\n",
      "Step taken: 0, New state: [ 2.8590259e-01  1.4426656e+00  9.5182484e-01 -7.4477278e-02\n",
      " -2.0094551e-01  5.4520165e-04  0.0000000e+00  0.0000000e+00], Reward: -0.18567194541765275, Done: False\n",
      "Step taken: 2, New state: [ 0.2957119   1.4415224   0.9802861  -0.05071885 -0.20029801  0.0129499\n",
      "  0.          0.        ], Reward: -3.0034100652311393, Done: False\n",
      "Step taken: 0, New state: [ 0.3055211   1.4397793   0.9802861  -0.07738554 -0.19965048  0.01295021\n",
      "  0.          0.        ], Reward: -0.13883867892457147, Done: False\n",
      "Step taken: 3, New state: [ 0.31539154  1.4374114   0.9880085  -0.10537089 -0.20061766 -0.0193436\n",
      "  0.          0.        ], Reward: -1.1310343029105911, Done: False\n",
      "Step taken: 2, New state: [ 0.32552776  1.435821    1.0142236  -0.07076251 -0.20122738 -0.01219459\n",
      "  0.          0.        ], Reward: -2.734200492237437, Done: False\n",
      "Step taken: 1, New state: [ 0.3355767   1.433656    1.0032573  -0.09600414 -0.19957773  0.03299265\n",
      "  0.          0.        ], Reward: 1.00522515546058, Done: False\n",
      "Step taken: 2, New state: [ 0.34574002  1.4316243   1.0146216  -0.09007243 -0.1978581   0.03439279\n",
      "  0.          0.        ], Reward: -1.2426445137298174, Done: False\n",
      "Step taken: 1, New state: [ 0.3558449   1.4290216   1.0072274  -0.11523993 -0.19457503  0.06566106\n",
      "  0.          0.        ], Reward: 0.7917301629661619, Done: False\n",
      "Step taken: 2, New state: [ 0.36604786  1.4264507   1.0169668  -0.11383113 -0.19122352  0.0670303\n",
      "  0.          0.        ], Reward: -0.9175936112380783, Done: False\n",
      "Step taken: 1, New state: [ 0.37617558  1.4232925   1.0075392  -0.13969342 -0.18596084  0.10525353\n",
      "  0.          0.        ], Reward: 1.160668208213708, Done: False\n",
      "Step taken: 1, New state: [ 0.38623372  1.4195445   0.99881095 -0.16572477 -0.17893612  0.14049408\n",
      "  0.          0.        ], Reward: 1.2450373307839595, Done: False\n",
      "Step taken: 3, New state: [ 0.39636928  1.4151809   1.0085231  -0.19333355 -0.17388858  0.10095099\n",
      "  0.          0.        ], Reward: -0.8164188212171826, Done: False\n",
      "Step taken: 2, New state: [ 0.40663528  1.4111375   1.0214579  -0.17911479 -0.16873789  0.10301389\n",
      "  0.          0.        ], Reward: -0.692163810010004, Done: False\n",
      "Step taken: 1, New state: [ 0.4168312   1.4065183   1.012624   -0.20452484 -0.16176344  0.13948886\n",
      "  0.          0.        ], Reward: 1.2218910497773277, Done: False\n",
      "Step taken: 0, New state: [ 0.42702723  1.4012997   1.0126232  -0.23119555 -0.15478903  0.13948841\n",
      "  0.          0.        ], Reward: 0.34291764710002326, Done: False\n",
      "Step taken: 2, New state: [ 0.43739778  1.3967001   1.029849   -0.20369402 -0.14759687  0.14384332\n",
      "  0.          0.        ], Reward: -0.5594064528943818, Done: False\n",
      "Step taken: 1, New state: [ 0.4476784   1.3915257   1.0185245  -0.22905819 -0.13808909  0.19015566\n",
      "  0.          0.        ], Reward: 1.686580508792049, Done: False\n",
      "Step taken: 0, New state: [ 0.4579591   1.3857527   1.018523   -0.25573236 -0.12858136  0.19015443\n",
      "  0.          0.        ], Reward: 0.5635476775221946, Done: False\n",
      "Step taken: 3, New state: [ 0.468332    1.3793548   1.0301213  -0.28374407 -0.12144241  0.14277911\n",
      "  0.          0.        ], Reward: -0.8737479896400078, Done: False\n",
      "Step taken: 3, New state: [ 0.47878075  1.3723441   1.0396327  -0.31116313 -0.1162286   0.10427592\n",
      "  0.          0.        ], Reward: -0.8572427858436231, Done: False\n",
      "Step taken: 1, New state: [ 0.48915178  1.3647487   1.029863   -0.33703354 -0.10903661  0.14384006\n",
      "  0.          0.        ], Reward: 1.2185526670594709, Done: False\n",
      "Step taken: 2, New state: [ 0.4996646   1.357267    1.0436277  -0.33197975 -0.10143224  0.15208742\n",
      "  0.          0.        ], Reward: -0.35008622944512807, Done: False\n",
      "Step taken: 1, New state: [ 0.5100973   1.3492032   1.0335476  -0.35775915 -0.09178559  0.19293293\n",
      "  0.          0.        ], Reward: 1.469717963272701, Done: False\n",
      "Step taken: 2, New state: [ 0.5207008   1.3417735   1.0501639  -0.32961223 -0.08170223  0.20166755\n",
      "  0.          0.        ], Reward: 0.3267044073432544, Done: False\n",
      "Step taken: 2, New state: [ 0.53126353  1.3348436   1.0465666  -0.30749968 -0.07208241  0.19239625\n",
      "  0.          0.        ], Reward: 1.9076188697988072, Done: False\n",
      "Step taken: 2, New state: [ 0.5419389   1.3279178   1.0574136  -0.307359   -0.06207851  0.20007789\n",
      "  0.          0.        ], Reward: -0.09351336699971852, Done: False\n",
      "Step taken: 3, New state: [ 0.55271214  1.320389    1.069697   -0.33431754 -0.05453663  0.15083788\n",
      "  0.          0.        ], Reward: -0.9460558696206067, Done: False\n",
      "Step taken: 0, New state: [ 0.56348544  1.3122609   1.0696965  -0.36098897 -0.04699479  0.15083733\n",
      "  0.          0.        ], Reward: 0.2576999729051863, Done: False\n",
      "Step taken: 2, New state: [ 0.5744383   1.3043681   1.0869418  -0.35054424 -0.03874448  0.16500603\n",
      "  0.          0.        ], Reward: -0.49840373693836, Done: False\n",
      "Step taken: 0, New state: [ 0.5853912   1.2958765   1.0869415  -0.37721664 -0.03049423  0.16500519\n",
      "  0.          0.        ], Reward: 0.30770189988095353, Done: False\n",
      "Step taken: 0, New state: [ 0.5963442   1.2867857   1.0869412  -0.40388903 -0.02224402  0.16500442\n",
      "  0.          0.        ], Reward: 0.29409821735777086, Done: False\n",
      "Step taken: 3, New state: [ 0.6073879   1.2771046   1.0983255  -0.43019712 -0.01627004  0.11947955\n",
      "  0.          0.        ], Reward: -1.0272569923073138, Done: False\n",
      "Step taken: 2, New state: [ 0.61839926  1.2674541   1.0952656  -0.42885596 -0.01047759  0.1158486\n",
      "  0.          0.        ], Reward: 1.0045914962589222, Done: False\n",
      "Step taken: 1, New state: [ 0.6293432   1.2572002   1.086827   -0.45569238 -0.00299603  0.1496315\n",
      "  0.          0.        ], Reward: 0.9264081522574383, Done: False\n",
      "Step taken: 1, New state: [ 0.6402136   1.2463527   1.0775777  -0.48212868  0.00633805  0.18668142\n",
      "  0.          0.        ], Reward: -0.0907169651372601, Done: False\n",
      "Step taken: 1, New state: [ 0.65099525  1.2348949   1.0664494  -0.5093321   0.01790178  0.23127511\n",
      "  0.          0.        ], Reward: -0.7994193673944483, Done: False\n",
      "Step taken: 0, New state: [ 0.6617769   1.2228389   1.0664496  -0.53600997  0.02946545  0.23127297\n",
      "  0.          0.        ], Reward: -1.7749549541648548, Done: False\n",
      "Step taken: 3, New state: [ 0.6726545   1.2101899   1.0784888  -0.56238294  0.03861572  0.18300556\n",
      "  0.          0.        ], Reward: -2.632693271697376, Done: False\n",
      "Step taken: 0, New state: [ 0.68353194  1.1969421   1.0784892  -0.5890566   0.04776596  0.18300474\n",
      "  0.          0.        ], Reward: -1.550858036088755, Done: False\n",
      "Step taken: 0, New state: [ 0.6944094   1.1830955   1.0784895  -0.6157303   0.05691613  0.18300372\n",
      "  0.          0.        ], Reward: -1.5624108286355636, Done: False\n",
      "Step taken: 1, New state: [ 0.7052128   1.1686504   1.069196   -0.6424698   0.06792589  0.22019513\n",
      "  0.          0.        ], Reward: -0.9919021195497908, Done: False\n",
      "Step taken: 0, New state: [ 0.716016    1.1536067   1.0691969  -0.6691467   0.07893555  0.2201933\n",
      "  0.          0.        ], Reward: -1.7766006677229598, Done: False\n",
      "Step taken: 0, New state: [ 0.7268192   1.1379647   1.069198   -0.6958235   0.08994512  0.22019145\n",
      "  0.          0.        ], Reward: -1.7882720172220274, Done: False\n",
      "Step taken: 3, New state: [ 0.7376873   1.1217214   1.0773379  -0.72252285  0.09933222  0.1877422\n",
      "  0.          0.        ], Reward: -2.3476764611969245, Done: False\n",
      "Step taken: 1, New state: [ 0.748489    1.1048741   1.0690222  -0.74955136  0.11038981  0.22115202\n",
      "  0.          0.        ], Reward: -1.1771022134981013, Done: False\n",
      "Step taken: 2, New state: [ 0.7592085   1.0889277   1.0606762  -0.7096026   0.12157874  0.22377852\n",
      "  0.          0.        ], Reward: 2.234408933975158, Done: False\n",
      "Step taken: 0, New state: [ 0.7699276   1.072383    1.060678   -0.7362797   0.13276757  0.22377658\n",
      "  0.          0.        ], Reward: -1.8899465145080399, Done: False\n",
      "Step taken: 2, New state: [ 0.7804127   1.0565023   1.0378394  -0.7067964   0.14339909  0.2126302\n",
      "  0.          0.        ], Reward: 2.8557351716901396, Done: False\n",
      "Step taken: 0, New state: [ 0.7908977   1.0400232   1.0378413  -0.73347247  0.15403052  0.21262851\n",
      "  0.          0.        ], Reward: -1.8941752079587104, Done: False\n",
      "Step taken: 2, New state: [ 0.80139315  1.0243329   1.0382121  -0.698566    0.16536663  0.22672221\n",
      "  0.          0.        ], Reward: 1.1191207740919935, Done: False\n",
      "Step taken: 1, New state: [ 0.81181276  1.0080163   1.02867    -0.7267152   0.17867969  0.26626053\n",
      "  0.          0.        ], Reward: -1.5435387894723067, Done: False\n",
      "Step taken: 0, New state: [ 0.82223207  0.99110216  1.0286738  -0.75339663  0.19199258  0.2662573\n",
      "  0.          0.        ], Reward: -2.239550159941416, Done: False\n",
      "Step taken: 0, New state: [ 0.83265096  0.9735901   1.0286777  -0.7800779   0.2053053   0.26625407\n",
      "  0.          0.        ], Reward: -2.257911633639651, Done: False\n",
      "Step taken: 1, New state: [ 0.84299964  0.9554471   1.0198171  -0.80851305  0.22049116  0.30371743\n",
      "  0.          0.        ], Reward: -1.8998457310819095, Done: False\n",
      "Step taken: 3, New state: [ 0.8534428   0.93673223  1.031725   -0.833705    0.23322771  0.25473154\n",
      "  0.          0.        ], Reward: -3.111273097339931, Done: False\n",
      "Step taken: 3, New state: [ 0.8639471   0.9174491   1.0395455  -0.8587906   0.24430192  0.22148445\n",
      "  0.          0.        ], Reward: -2.629373466751615, Done: False\n",
      "Step taken: 1, New state: [ 0.87436736  0.8975274   1.0289102  -0.88763976  0.25764206  0.26680282\n",
      "  0.          0.        ], Reward: -1.6945018159919119, Done: False\n",
      "Step taken: 2, New state: [ 0.8845377   0.8777737   1.0044868  -0.88019663  0.27046895  0.2565382\n",
      "  0.          0.        ], Reward: 1.4362138502935522, Done: False\n",
      "Step taken: 1, New state: [ 0.89463866  0.8573958   0.9958135  -0.9083847   0.28512275  0.29307598\n",
      "  0.          0.        ], Reward: -2.0280206320262253, Done: False\n",
      "Step taken: 2, New state: [ 0.9045593   0.8378948   0.9773978  -0.86965734  0.3002572   0.30268866\n",
      "  0.          0.        ], Reward: 2.7622949085943675, Done: False\n",
      "Step taken: 3, New state: [ 0.91457176  0.8178279   0.98894083 -0.8944489   0.31297347  0.2543256\n",
      "  0.          0.        ], Reward: -3.2061242258901346, Done: False\n",
      "Step taken: 2, New state: [ 0.9243723   0.7978532   0.9680603  -0.89039314  0.32541424  0.24881527\n",
      "  0.          0.        ], Reward: 0.8540955259147267, Done: False\n",
      "Step taken: 0, New state: [ 0.9341723   0.77728045  0.9680665  -0.9170722   0.3378548   0.24881259\n",
      "  0.          0.        ], Reward: -2.4825608536505115, Done: False\n",
      "Step taken: 0, New state: [ 0.9439718   0.7561097   0.9680728  -0.94375116  0.35029528  0.24880993\n",
      "  0.          0.        ], Reward: -2.513380056450103, Done: False\n",
      "Step taken: 3, New state: [ 0.95383376  0.7343694   0.9759819  -0.9687582   0.36103258  0.21474668\n",
      "  0.          0.        ], Reward: -2.8540820233620523, Done: False\n",
      "Step taken: 2, New state: [ 0.9635769   0.713033    0.9636067  -0.9510184   0.37234926  0.22633395\n",
      "  0.          0.        ], Reward: 1.2036410393939831, Done: False\n",
      "Step taken: 1, New state: [ 0.9732588   0.6910757   0.9560097  -0.979106    0.3852776   0.25856704\n",
      "  0.          0.        ], Reward: -2.27368639653386, Done: False\n",
      "Step taken: 3, New state: [ 0.98301697  0.66856056  0.9657119  -1.0034406   0.39607397  0.2159271\n",
      "  0.          0.        ], Reward: -3.048574365041445, Done: False\n",
      "Step taken: 3, New state: [ 0.99285734  0.64550316  0.9761742  -1.0269867   0.40448204  0.16816145\n",
      "  0.          0.        ], Reward: -2.8381228051692617, Done: False\n",
      "Step taken: 1, New state: [ 1.002606    0.62179416  0.9646865  -1.056679    0.41545442  0.2194475\n",
      "  0.          0.        ], Reward: -100, Done: True\n",
      "Episode finished, score: -191.07676655732004\n",
      "Environment closed.\n"
     ]
    }
   ],
   "source": [
    "test_lunar_lander(steps_to_run_before_pause=0, agent=None, episodes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cpu\n"
     ]
    }
   ],
   "source": [
    "from DQN import DQNAgent\n",
    "lunar = LunarLanderEnv(render_mode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNetwork:\n",
      " DQN(\n",
      "  (fc1): Linear(in_features=8, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (out): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n",
      "Ep    0 | Reward:  -257.42 | Eps: 0.995 | Loss: 3.8808\n",
      "Ep    1 | Reward:  -399.07 | Eps: 0.990 | Loss: 3.7611\n",
      "Ep    2 | Reward:   -85.56 | Eps: 0.985 | Loss: 131.9518\n",
      "Ep    3 | Reward:  -322.75 | Eps: 0.980 | Loss: 28.4065\n",
      "Ep    4 | Reward:  -100.22 | Eps: 0.975 | Loss: 2.5027\n",
      "Ep    5 | Reward:  -269.85 | Eps: 0.970 | Loss: 127.1925\n",
      "Ep    6 | Reward:   -66.32 | Eps: 0.966 | Loss: 1.8813\n",
      "Ep    7 | Reward:   -43.90 | Eps: 0.961 | Loss: 25.6503\n",
      "Ep    8 | Reward:   -43.47 | Eps: 0.956 | Loss: 83.4885\n",
      "Ep    9 | Reward:  -107.14 | Eps: 0.951 | Loss: 8.1397\n",
      "Ep   10 | Reward:  -153.88 | Eps: 0.946 | Loss: 220.8284\n",
      "Ep   11 | Reward:  -349.91 | Eps: 0.942 | Loss: 117.6967\n",
      "Ep   12 | Reward:  -266.90 | Eps: 0.937 | Loss: 8.3578\n",
      "Ep   13 | Reward:   -77.28 | Eps: 0.932 | Loss: 68.8671\n",
      "Ep   14 | Reward:  -408.32 | Eps: 0.928 | Loss: 30.6387\n",
      "Ep   15 | Reward:   -15.07 | Eps: 0.923 | Loss: 4.9079\n",
      "Ep   16 | Reward:  -111.96 | Eps: 0.918 | Loss: 105.4291\n",
      "Ep   17 | Reward:  -249.07 | Eps: 0.914 | Loss: 27.6109\n",
      "Ep   18 | Reward:  -170.74 | Eps: 0.909 | Loss: 109.8841\n",
      "Ep   19 | Reward:  -100.47 | Eps: 0.905 | Loss: 52.4060\n",
      "Ep   20 | Reward:   -84.66 | Eps: 0.900 | Loss: 9.9266\n",
      "Ep   21 | Reward:  -118.84 | Eps: 0.896 | Loss: 46.7186\n",
      "Ep   22 | Reward:   -69.43 | Eps: 0.891 | Loss: 104.2419\n",
      "Ep   23 | Reward:   -68.77 | Eps: 0.887 | Loss: 82.0232\n",
      "Ep   24 | Reward:   -67.90 | Eps: 0.882 | Loss: 19.4824\n",
      "Ep   25 | Reward:  -320.53 | Eps: 0.878 | Loss: 83.3540\n",
      "Ep   26 | Reward:  -143.02 | Eps: 0.873 | Loss: 55.6856\n",
      "Ep   27 | Reward:   -77.63 | Eps: 0.869 | Loss: 2.2948\n",
      "Ep   28 | Reward:  -183.92 | Eps: 0.865 | Loss: 150.3908\n",
      "Ep   29 | Reward:  -129.13 | Eps: 0.860 | Loss: 54.4155\n",
      "Ep   30 | Reward:   -87.25 | Eps: 0.856 | Loss: 7.8824\n",
      "Ep   31 | Reward:  -116.95 | Eps: 0.852 | Loss: 38.0953\n",
      "Ep   32 | Reward:  -158.07 | Eps: 0.848 | Loss: 5.3502\n",
      "Ep   33 | Reward:  -161.69 | Eps: 0.843 | Loss: 195.9749\n",
      "Ep   34 | Reward:   -82.29 | Eps: 0.839 | Loss: 3.4221\n",
      "Ep   35 | Reward:  -283.96 | Eps: 0.835 | Loss: 1.4989\n",
      "Ep   36 | Reward:   -99.54 | Eps: 0.831 | Loss: 0.9887\n",
      "Ep   37 | Reward:  -145.08 | Eps: 0.827 | Loss: 14.2374\n",
      "Ep   38 | Reward:  -157.18 | Eps: 0.822 | Loss: 14.3744\n",
      "Ep   39 | Reward:  -254.43 | Eps: 0.818 | Loss: 5.7953\n",
      "Ep   40 | Reward:   -19.57 | Eps: 0.814 | Loss: 6.6973\n",
      "Ep   41 | Reward:  -118.51 | Eps: 0.810 | Loss: 84.1966\n",
      "Ep   42 | Reward:  -132.86 | Eps: 0.806 | Loss: 89.1390\n",
      "Ep   43 | Reward:  -103.99 | Eps: 0.802 | Loss: 3.8647\n",
      "Ep   44 | Reward:   -65.88 | Eps: 0.798 | Loss: 21.8527\n",
      "Ep   45 | Reward:  -108.75 | Eps: 0.794 | Loss: 19.3124\n",
      "Ep   46 | Reward:  -144.12 | Eps: 0.790 | Loss: 26.8168\n",
      "Ep   47 | Reward:   -44.66 | Eps: 0.786 | Loss: 20.5678\n",
      "Ep   48 | Reward:  -109.87 | Eps: 0.782 | Loss: 42.0305\n",
      "Ep   49 | Reward:   -80.64 | Eps: 0.778 | Loss: 80.3219\n",
      "Ep   50 | Reward:   -72.94 | Eps: 0.774 | Loss: 24.9619\n",
      "Ep   51 | Reward:  -151.20 | Eps: 0.771 | Loss: 155.6590\n",
      "Ep   52 | Reward:   -30.17 | Eps: 0.767 | Loss: 81.5232\n",
      "Ep   53 | Reward:  -116.55 | Eps: 0.763 | Loss: 15.2886\n",
      "Ep   54 | Reward:  -118.65 | Eps: 0.759 | Loss: 7.6930\n",
      "Ep   55 | Reward:   -27.62 | Eps: 0.755 | Loss: 1.9751\n",
      "Ep   56 | Reward:    -5.35 | Eps: 0.751 | Loss: 35.8172\n",
      "Ep   57 | Reward:  -158.60 | Eps: 0.748 | Loss: 9.7672\n",
      "Ep   58 | Reward:  -218.47 | Eps: 0.744 | Loss: 1.5861\n",
      "Ep   59 | Reward:  -251.65 | Eps: 0.740 | Loss: 94.5923\n",
      "Ep   60 | Reward:  -108.44 | Eps: 0.737 | Loss: 1.9684\n",
      "Ep   61 | Reward:   -38.11 | Eps: 0.733 | Loss: 15.7635\n",
      "Ep   62 | Reward:  -115.65 | Eps: 0.729 | Loss: 8.5661\n",
      "Ep   63 | Reward:   -84.91 | Eps: 0.726 | Loss: 20.3131\n",
      "Ep   64 | Reward:     2.19 | Eps: 0.722 | Loss: 23.0665\n",
      "Ep   65 | Reward:  -310.62 | Eps: 0.718 | Loss: 6.4314\n",
      "Ep   66 | Reward:  -166.25 | Eps: 0.715 | Loss: 11.0474\n",
      "Ep   67 | Reward:  -122.49 | Eps: 0.711 | Loss: 11.4678\n",
      "Ep   68 | Reward:  -140.28 | Eps: 0.708 | Loss: 11.2101\n",
      "Ep   69 | Reward:   -96.06 | Eps: 0.704 | Loss: 10.9604\n",
      "Ep   70 | Reward:   -54.95 | Eps: 0.701 | Loss: 14.0848\n",
      "Ep   71 | Reward:   -22.97 | Eps: 0.697 | Loss: 180.8676\n",
      "Ep   72 | Reward:   -93.21 | Eps: 0.694 | Loss: 271.5733\n",
      "Ep   73 | Reward:  -115.03 | Eps: 0.690 | Loss: 115.0400\n",
      "Ep   74 | Reward:  -114.63 | Eps: 0.687 | Loss: 82.7695\n",
      "Ep   75 | Reward:   -80.84 | Eps: 0.683 | Loss: 1.6290\n",
      "Ep   76 | Reward:  -205.83 | Eps: 0.680 | Loss: 4.9729\n",
      "Ep   77 | Reward:  -275.28 | Eps: 0.676 | Loss: 5.6806\n",
      "Ep   78 | Reward:  -164.16 | Eps: 0.673 | Loss: 9.0670\n",
      "Ep   79 | Reward:   -83.18 | Eps: 0.670 | Loss: 16.6795\n",
      "Ep   80 | Reward:   -66.58 | Eps: 0.666 | Loss: 3.1231\n",
      "Ep   81 | Reward:  -281.37 | Eps: 0.663 | Loss: 9.9213\n",
      "Ep   82 | Reward:  -136.25 | Eps: 0.660 | Loss: 2.2957\n",
      "Ep   83 | Reward:  -121.10 | Eps: 0.656 | Loss: 7.5471\n",
      "Ep   84 | Reward:   -89.17 | Eps: 0.653 | Loss: 12.0798\n",
      "Ep   85 | Reward:  -176.18 | Eps: 0.650 | Loss: 24.9022\n",
      "Ep   86 | Reward:   -57.30 | Eps: 0.647 | Loss: 63.6764\n",
      "Ep   87 | Reward:   -52.19 | Eps: 0.643 | Loss: 4.8228\n",
      "Ep   88 | Reward:     1.42 | Eps: 0.640 | Loss: 3.9279\n",
      "Ep   89 | Reward:   -69.63 | Eps: 0.637 | Loss: 5.4728\n",
      "Ep   90 | Reward:   -92.61 | Eps: 0.634 | Loss: 14.4862\n",
      "Ep   91 | Reward:   -56.31 | Eps: 0.631 | Loss: 38.4928\n",
      "Ep   92 | Reward:  -145.76 | Eps: 0.627 | Loss: 2.1496\n",
      "Ep   93 | Reward:  -305.38 | Eps: 0.624 | Loss: 4.9433\n",
      "Ep   94 | Reward:   -51.27 | Eps: 0.621 | Loss: 7.4557\n",
      "Ep   95 | Reward:   -68.14 | Eps: 0.618 | Loss: 20.1214\n",
      "Ep   96 | Reward:    21.53 | Eps: 0.615 | Loss: 2.8834\n",
      "Ep   97 | Reward:   -76.05 | Eps: 0.612 | Loss: 91.2081\n",
      "Ep   98 | Reward:   -73.82 | Eps: 0.609 | Loss: 8.6826\n",
      "Ep   99 | Reward:   -75.15 | Eps: 0.606 | Loss: 2.8992\n",
      "Ep  100 | Reward:  -205.55 | Eps: 0.603 | Loss: 28.0556\n",
      "Ep  101 | Reward:   -35.08 | Eps: 0.600 | Loss: 28.2028\n",
      "Ep  102 | Reward:  -321.30 | Eps: 0.597 | Loss: 4.4730\n",
      "Ep  103 | Reward:  -103.35 | Eps: 0.594 | Loss: 11.4957\n",
      "Ep  104 | Reward:   -31.46 | Eps: 0.591 | Loss: 7.3078\n",
      "Ep  105 | Reward:  -158.05 | Eps: 0.588 | Loss: 7.3045\n",
      "Ep  106 | Reward:   -88.84 | Eps: 0.585 | Loss: 46.0175\n",
      "Ep  107 | Reward:   -58.01 | Eps: 0.582 | Loss: 4.9368\n",
      "Ep  108 | Reward:   -21.51 | Eps: 0.579 | Loss: 25.9240\n",
      "Ep  109 | Reward:    -3.03 | Eps: 0.576 | Loss: 13.7628\n",
      "Ep  110 | Reward:  -103.14 | Eps: 0.573 | Loss: 10.9077\n",
      "Ep  111 | Reward:   -57.57 | Eps: 0.570 | Loss: 8.1076\n",
      "Ep  112 | Reward:   -48.36 | Eps: 0.568 | Loss: 16.0347\n",
      "Ep  113 | Reward:  -127.06 | Eps: 0.565 | Loss: 18.8870\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m agent = DQNAgent(lunar,episodes=\u001b[32m2000\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#añado yo\u001b[39;00m\n\u001b[32m      4\u001b[39m agent.save_model(\u001b[33m\"\u001b[39m\u001b[33mmodelo_DQN.h5\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\LunarEntorno\\DQN.py:229\u001b[39m, in \u001b[36mDQNAgent.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    226\u001b[39m total_reward=\u001b[32m0\u001b[39m\n\u001b[32m    227\u001b[39m done=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m    230\u001b[39m     \u001b[38;5;66;03m#eligumos una accion y la ejecutamos\u001b[39;00m\n\u001b[32m    231\u001b[39m     next_state, reward, done, action = \u001b[38;5;28mself\u001b[39m.act()\n\u001b[32m    232\u001b[39m     \u001b[38;5;28mself\u001b[39m.memory.push(state, action, reward, next_state, done)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(lunar)\n",
    "agent.train()\n",
    "#añado yo\n",
    "agent.save_model(\"modelo_DQN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNetwork:\n",
      " DQN(\n",
      "  (fc1): Linear(in_features=8, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (out): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(\n",
    "    lunar,\n",
    "    \n",
    "    target_network_update_freq=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep    0 | Reward:  -136.81 | Eps: 0.997 | Loss: 0.0000\n",
      "Ep    1 | Reward:  -101.23 | Eps: 0.994 | Loss: 10.1573\n",
      "Ep    2 | Reward:  -227.41 | Eps: 0.991 | Loss: 161.2987\n",
      "Ep    3 | Reward:   -62.67 | Eps: 0.988 | Loss: 36.6249\n",
      "Ep    4 | Reward:  -381.77 | Eps: 0.985 | Loss: 2.5397\n",
      "Ep    5 | Reward:  -151.65 | Eps: 0.982 | Loss: 5.2538\n",
      "Ep    6 | Reward:  -396.68 | Eps: 0.979 | Loss: 57.7551\n",
      "Ep    7 | Reward:   -91.34 | Eps: 0.976 | Loss: 12.4495\n",
      "Ep    8 | Reward:  -254.72 | Eps: 0.973 | Loss: 78.0045\n",
      "Ep    9 | Reward:  -113.37 | Eps: 0.970 | Loss: 28.2858\n",
      "Ep   10 | Reward:  -107.27 | Eps: 0.967 | Loss: 19.4819\n",
      "Ep   11 | Reward:   -14.11 | Eps: 0.965 | Loss: 51.8114\n",
      "Ep   12 | Reward:  -373.74 | Eps: 0.962 | Loss: 55.8290\n",
      "Ep   13 | Reward:  -130.52 | Eps: 0.959 | Loss: 316.1292\n",
      "Ep   14 | Reward:  -103.06 | Eps: 0.956 | Loss: 51.1635\n",
      "Ep   15 | Reward:   -64.02 | Eps: 0.953 | Loss: 27.0938\n",
      "Ep   16 | Reward:  -175.00 | Eps: 0.950 | Loss: 171.3437\n",
      "Ep   17 | Reward:  -106.21 | Eps: 0.947 | Loss: 83.6193\n",
      "Ep   18 | Reward:  -103.49 | Eps: 0.945 | Loss: 44.1462\n",
      "Ep   19 | Reward:  -111.56 | Eps: 0.942 | Loss: 33.7103\n",
      "Ep   20 | Reward:  -231.48 | Eps: 0.939 | Loss: 34.8350\n",
      "Ep   21 | Reward:  -145.19 | Eps: 0.936 | Loss: 79.9643\n",
      "Ep   22 | Reward:  -227.90 | Eps: 0.933 | Loss: 45.3419\n",
      "Ep   23 | Reward:  -176.89 | Eps: 0.930 | Loss: 21.2832\n",
      "Ep   24 | Reward:   -73.52 | Eps: 0.928 | Loss: 469.1683\n",
      "Ep   25 | Reward:  -107.08 | Eps: 0.925 | Loss: 32.6046\n",
      "Ep   26 | Reward:  -104.14 | Eps: 0.922 | Loss: 6.3698\n",
      "Ep   27 | Reward:  -111.11 | Eps: 0.919 | Loss: 16.5709\n",
      "Ep   28 | Reward:  -339.75 | Eps: 0.917 | Loss: 381.1458\n",
      "Ep   29 | Reward:  -109.63 | Eps: 0.914 | Loss: 80.8449\n",
      "Ep   30 | Reward:   -85.51 | Eps: 0.911 | Loss: 68.7429\n",
      "Ep   31 | Reward:   -82.37 | Eps: 0.908 | Loss: 31.8456\n",
      "Ep   32 | Reward:   -83.23 | Eps: 0.906 | Loss: 99.2866\n",
      "Ep   33 | Reward:  -289.29 | Eps: 0.903 | Loss: 87.6082\n",
      "Ep   34 | Reward:  -137.83 | Eps: 0.900 | Loss: 180.3482\n",
      "Ep   35 | Reward:  -104.69 | Eps: 0.897 | Loss: 56.9538\n",
      "Ep   36 | Reward:  -102.80 | Eps: 0.895 | Loss: 48.5840\n",
      "Ep   37 | Reward:  -107.07 | Eps: 0.892 | Loss: 17.4407\n",
      "Ep   38 | Reward:  -105.32 | Eps: 0.889 | Loss: 74.4574\n",
      "Ep   39 | Reward:   -23.22 | Eps: 0.887 | Loss: 19.6121\n",
      "Ep   40 | Reward:  -117.62 | Eps: 0.884 | Loss: 30.0868\n",
      "Ep   41 | Reward:   -72.78 | Eps: 0.881 | Loss: 19.2797\n",
      "Ep   42 | Reward:  -125.10 | Eps: 0.879 | Loss: 18.1027\n",
      "Ep   43 | Reward:  -210.86 | Eps: 0.876 | Loss: 37.7311\n",
      "Ep   44 | Reward:  -125.81 | Eps: 0.874 | Loss: 42.9223\n",
      "Ep   45 | Reward:  -111.56 | Eps: 0.871 | Loss: 64.0789\n",
      "Ep   46 | Reward:  -144.04 | Eps: 0.868 | Loss: 24.0440\n",
      "Ep   47 | Reward:  -147.07 | Eps: 0.866 | Loss: 96.9677\n",
      "Ep   48 | Reward:  -257.89 | Eps: 0.863 | Loss: 30.1606\n",
      "Ep   49 | Reward:  -221.46 | Eps: 0.861 | Loss: 31.5530\n",
      "Ep   50 | Reward:  -309.83 | Eps: 0.858 | Loss: 31.3034\n",
      "Ep   51 | Reward:   -98.50 | Eps: 0.855 | Loss: 19.5529\n",
      "Ep   52 | Reward:    -8.15 | Eps: 0.853 | Loss: 12.6335\n",
      "Ep   53 | Reward:  -117.42 | Eps: 0.850 | Loss: 55.7663\n",
      "Ep   54 | Reward:   -90.49 | Eps: 0.848 | Loss: 21.2267\n",
      "Ep   55 | Reward:  -103.29 | Eps: 0.845 | Loss: 72.6953\n",
      "Ep   56 | Reward:  -181.94 | Eps: 0.843 | Loss: 37.0003\n",
      "Ep   57 | Reward:  -158.85 | Eps: 0.840 | Loss: 69.0201\n",
      "Ep   58 | Reward:  -108.15 | Eps: 0.838 | Loss: 14.5825\n",
      "Ep   59 | Reward:  -109.89 | Eps: 0.835 | Loss: 7.8008\n",
      "Ep   60 | Reward:   -77.54 | Eps: 0.833 | Loss: 69.7480\n",
      "Ep   61 | Reward:  -178.17 | Eps: 0.830 | Loss: 7.4393\n",
      "Ep   62 | Reward:  -151.71 | Eps: 0.828 | Loss: 20.6285\n",
      "Ep   63 | Reward:   -88.63 | Eps: 0.825 | Loss: 64.6455\n",
      "Ep   64 | Reward:   -55.25 | Eps: 0.823 | Loss: 137.0467\n",
      "Ep   65 | Reward:  -147.26 | Eps: 0.820 | Loss: 14.5486\n",
      "Ep   66 | Reward:  -143.01 | Eps: 0.818 | Loss: 69.3717\n",
      "Ep   67 | Reward:  -124.18 | Eps: 0.815 | Loss: 15.7196\n",
      "Ep   68 | Reward:   -58.82 | Eps: 0.813 | Loss: 6.0467\n",
      "Ep   69 | Reward:   -63.40 | Eps: 0.810 | Loss: 37.1577\n",
      "Ep   70 | Reward:  -204.20 | Eps: 0.808 | Loss: 56.0002\n",
      "Ep   71 | Reward:  -114.98 | Eps: 0.805 | Loss: 43.6485\n",
      "Ep   72 | Reward:   -73.60 | Eps: 0.803 | Loss: 9.1720\n",
      "Ep   73 | Reward:   -86.45 | Eps: 0.801 | Loss: 86.6988\n",
      "Ep   74 | Reward:  -112.93 | Eps: 0.798 | Loss: 26.2427\n",
      "Ep   75 | Reward:  -108.94 | Eps: 0.796 | Loss: 14.5371\n",
      "Ep   76 | Reward:  -108.30 | Eps: 0.793 | Loss: 30.5241\n",
      "Ep   77 | Reward:   -54.31 | Eps: 0.791 | Loss: 23.6393\n",
      "Ep   78 | Reward:  -101.98 | Eps: 0.789 | Loss: 17.3283\n",
      "Ep   79 | Reward:  -100.34 | Eps: 0.786 | Loss: 55.6109\n",
      "Ep   80 | Reward:   -54.63 | Eps: 0.784 | Loss: 9.0054\n",
      "Ep   81 | Reward:  -207.11 | Eps: 0.782 | Loss: 143.6431\n",
      "Ep   82 | Reward:  -104.01 | Eps: 0.779 | Loss: 143.3918\n",
      "Ep   83 | Reward:  -165.84 | Eps: 0.777 | Loss: 14.2904\n",
      "Ep   84 | Reward:  -151.15 | Eps: 0.775 | Loss: 40.6291\n",
      "Ep   85 | Reward:   -33.26 | Eps: 0.772 | Loss: 17.6481\n",
      "Ep   86 | Reward:  -145.04 | Eps: 0.770 | Loss: 87.4218\n",
      "Ep   87 | Reward:   -79.97 | Eps: 0.768 | Loss: 55.6013\n",
      "Ep   88 | Reward:  -100.79 | Eps: 0.765 | Loss: 47.6852\n",
      "Ep   89 | Reward:   -88.24 | Eps: 0.763 | Loss: 33.8458\n",
      "Ep   90 | Reward:   -76.20 | Eps: 0.761 | Loss: 26.2948\n",
      "Ep   91 | Reward:   -23.02 | Eps: 0.758 | Loss: 22.7267\n",
      "Ep   92 | Reward:   -56.54 | Eps: 0.756 | Loss: 14.6712\n",
      "Ep   93 | Reward:   -61.96 | Eps: 0.754 | Loss: 135.7016\n",
      "Ep   94 | Reward:   -56.96 | Eps: 0.752 | Loss: 87.8431\n",
      "Ep   95 | Reward:  -136.12 | Eps: 0.749 | Loss: 15.6238\n",
      "Ep   96 | Reward:   -36.46 | Eps: 0.747 | Loss: 63.9152\n",
      "Ep   97 | Reward:   -55.63 | Eps: 0.745 | Loss: 63.9495\n",
      "Ep   98 | Reward:   -90.40 | Eps: 0.743 | Loss: 80.6085\n",
      "Ep   99 | Reward:   -88.64 | Eps: 0.740 | Loss: 22.8378\n",
      "Ep  100 | Reward:  -139.47 | Eps: 0.738 | Loss: 38.8826\n",
      "Ep  101 | Reward:   -91.71 | Eps: 0.736 | Loss: 41.9447\n",
      "Ep  102 | Reward:   -24.22 | Eps: 0.734 | Loss: 49.4740\n",
      "Ep  103 | Reward:  -110.13 | Eps: 0.732 | Loss: 70.8616\n",
      "Ep  104 | Reward:   -87.52 | Eps: 0.729 | Loss: 19.1558\n",
      "Ep  105 | Reward:  -126.97 | Eps: 0.727 | Loss: 23.2983\n",
      "Ep  106 | Reward:  -118.01 | Eps: 0.725 | Loss: 30.7164\n",
      "Ep  107 | Reward:  -115.53 | Eps: 0.723 | Loss: 63.7886\n",
      "Ep  108 | Reward:   -56.07 | Eps: 0.721 | Loss: 9.0824\n",
      "Ep  109 | Reward:   -82.69 | Eps: 0.719 | Loss: 83.8591\n",
      "Ep  110 | Reward:   -64.30 | Eps: 0.716 | Loss: 25.0242\n",
      "Ep  111 | Reward:   -64.93 | Eps: 0.714 | Loss: 51.5964\n",
      "Ep  112 | Reward:   -39.35 | Eps: 0.712 | Loss: 121.5830\n",
      "Ep  113 | Reward:  -108.92 | Eps: 0.710 | Loss: 87.1539\n",
      "Ep  114 | Reward:  -121.09 | Eps: 0.708 | Loss: 44.4378\n",
      "Ep  115 | Reward:  -103.35 | Eps: 0.706 | Loss: 25.7894\n",
      "Ep  116 | Reward:   -83.09 | Eps: 0.704 | Loss: 47.3765\n",
      "Ep  117 | Reward:  -131.27 | Eps: 0.702 | Loss: 16.8608\n",
      "Ep  118 | Reward:   -54.46 | Eps: 0.699 | Loss: 21.2844\n",
      "Ep  119 | Reward:   -62.12 | Eps: 0.697 | Loss: 32.8454\n",
      "Ep  120 | Reward:   -78.63 | Eps: 0.695 | Loss: 143.7577\n",
      "Ep  121 | Reward:   -94.46 | Eps: 0.693 | Loss: 139.2319\n",
      "Ep  122 | Reward:  -113.50 | Eps: 0.691 | Loss: 85.8282\n",
      "Ep  123 | Reward:   -89.48 | Eps: 0.689 | Loss: 47.4990\n",
      "Ep  124 | Reward:   -87.07 | Eps: 0.687 | Loss: 45.9519\n",
      "Ep  125 | Reward:   -49.36 | Eps: 0.685 | Loss: 62.2021\n",
      "Ep  126 | Reward:   -53.23 | Eps: 0.683 | Loss: 28.0604\n",
      "Ep  127 | Reward:  -108.57 | Eps: 0.681 | Loss: 22.8724\n",
      "Ep  128 | Reward:   -38.95 | Eps: 0.679 | Loss: 22.4096\n",
      "Ep  129 | Reward:   -78.89 | Eps: 0.677 | Loss: 99.3017\n",
      "Ep  130 | Reward:  -113.54 | Eps: 0.675 | Loss: 37.2434\n",
      "Ep  131 | Reward:   -75.12 | Eps: 0.673 | Loss: 41.5087\n",
      "Ep  132 | Reward:   -43.66 | Eps: 0.671 | Loss: 120.5250\n",
      "Ep  133 | Reward:   -50.91 | Eps: 0.669 | Loss: 452.8591\n",
      "Ep  134 | Reward:  -216.03 | Eps: 0.667 | Loss: 100.6936\n",
      "Ep  135 | Reward:   -73.01 | Eps: 0.665 | Loss: 191.9734\n",
      "Ep  136 | Reward:   -80.41 | Eps: 0.663 | Loss: 33.6048\n",
      "Ep  137 | Reward:   -70.66 | Eps: 0.661 | Loss: 168.3063\n",
      "Ep  138 | Reward:   -99.67 | Eps: 0.659 | Loss: 43.2890\n",
      "Ep  139 | Reward:  -106.87 | Eps: 0.657 | Loss: 367.5183\n",
      "Ep  140 | Reward:  -120.20 | Eps: 0.655 | Loss: 42.0999\n",
      "Ep  141 | Reward:   -82.69 | Eps: 0.653 | Loss: 40.2968\n",
      "Ep  142 | Reward:   -98.61 | Eps: 0.651 | Loss: 53.4375\n",
      "Ep  143 | Reward:   -78.21 | Eps: 0.649 | Loss: 591.0215\n",
      "Ep  144 | Reward:   -95.42 | Eps: 0.647 | Loss: 326.8560\n",
      "Ep  145 | Reward:   -72.47 | Eps: 0.645 | Loss: 80.6435\n",
      "Ep  146 | Reward:   -51.39 | Eps: 0.643 | Loss: 59.9293\n",
      "Ep  147 | Reward:   -60.81 | Eps: 0.641 | Loss: 381.2790\n",
      "Ep  148 | Reward:  -149.45 | Eps: 0.639 | Loss: 38.4327\n",
      "Ep  149 | Reward:  -112.25 | Eps: 0.637 | Loss: 78.5495\n",
      "Ep  150 | Reward:   -41.51 | Eps: 0.635 | Loss: 130.4078\n",
      "Ep  151 | Reward:  -117.82 | Eps: 0.633 | Loss: 2115.4890\n",
      "Ep  152 | Reward:  -113.18 | Eps: 0.631 | Loss: 516.9651\n",
      "Ep  153 | Reward:     0.37 | Eps: 0.630 | Loss: 2092.0151\n",
      "Ep  154 | Reward:  -109.06 | Eps: 0.628 | Loss: 411.9483\n",
      "Ep  155 | Reward:   -67.68 | Eps: 0.626 | Loss: 106.0771\n",
      "Ep  156 | Reward:   -34.26 | Eps: 0.624 | Loss: 398.0091\n",
      "Ep  157 | Reward:   -37.78 | Eps: 0.622 | Loss: 89.2021\n",
      "Ep  158 | Reward:  -150.75 | Eps: 0.620 | Loss: 132.5551\n",
      "Ep  159 | Reward:  -138.67 | Eps: 0.618 | Loss: 445.8455\n",
      "Ep  160 | Reward:   -67.12 | Eps: 0.616 | Loss: 206.1483\n",
      "Ep  161 | Reward:   -61.24 | Eps: 0.615 | Loss: 211.0944\n",
      "Ep  162 | Reward:   -62.10 | Eps: 0.613 | Loss: 48.7913\n",
      "Ep  163 | Reward:  -187.29 | Eps: 0.611 | Loss: 1507.9327\n",
      "Ep  164 | Reward:   -57.66 | Eps: 0.609 | Loss: 64.4189\n",
      "Ep  165 | Reward:   -92.37 | Eps: 0.607 | Loss: 48.0426\n",
      "Ep  166 | Reward:  -251.96 | Eps: 0.605 | Loss: 327.8523\n",
      "Ep  167 | Reward:   -92.55 | Eps: 0.604 | Loss: 50.2452\n",
      "Ep  168 | Reward:   -66.88 | Eps: 0.602 | Loss: 10677.6377\n",
      "Ep  169 | Reward:  -205.50 | Eps: 0.600 | Loss: 320.3460\n",
      "Ep  170 | Reward:   -87.72 | Eps: 0.598 | Loss: 177.8403\n",
      "Ep  171 | Reward:  -102.09 | Eps: 0.596 | Loss: 176.1821\n",
      "Ep  172 | Reward:   -69.51 | Eps: 0.595 | Loss: 2814.8008\n",
      "Ep  173 | Reward:  -248.48 | Eps: 0.593 | Loss: 367.4563\n",
      "Ep  174 | Reward:  -179.16 | Eps: 0.591 | Loss: 239.8469\n",
      "Ep  175 | Reward:  -179.70 | Eps: 0.589 | Loss: 8232.2334\n",
      "Ep  176 | Reward:  -522.37 | Eps: 0.588 | Loss: 9648.4355\n",
      "Ep  177 | Reward:   -33.16 | Eps: 0.586 | Loss: 586.1188\n",
      "Ep  178 | Reward:   -43.40 | Eps: 0.584 | Loss: 3018.3564\n",
      "Ep  179 | Reward:  -324.89 | Eps: 0.582 | Loss: 5583.7173\n",
      "Ep  180 | Reward:  -136.99 | Eps: 0.581 | Loss: 8191.4014\n",
      "Ep  181 | Reward:  -110.55 | Eps: 0.579 | Loss: 357.6478\n",
      "Ep  182 | Reward:  -103.81 | Eps: 0.577 | Loss: 295.3419\n",
      "Ep  183 | Reward:  -330.71 | Eps: 0.575 | Loss: 5383.4917\n",
      "Ep  184 | Reward:   -69.19 | Eps: 0.574 | Loss: 2232.7749\n",
      "Ep  185 | Reward:  -239.28 | Eps: 0.572 | Loss: 712.8654\n",
      "Ep  186 | Reward:  -107.96 | Eps: 0.570 | Loss: 239.3732\n",
      "Ep  187 | Reward:   -66.74 | Eps: 0.568 | Loss: 760.8068\n",
      "Ep  188 | Reward:  -234.38 | Eps: 0.567 | Loss: 15304.0938\n",
      "Ep  189 | Reward:  -153.30 | Eps: 0.565 | Loss: 934630.1875\n",
      "Ep  190 | Reward:  -141.71 | Eps: 0.563 | Loss: 841.4197\n",
      "Ep  191 | Reward:  -118.40 | Eps: 0.562 | Loss: 13675.8379\n",
      "Ep  192 | Reward:   -92.53 | Eps: 0.560 | Loss: 2771.2026\n",
      "Ep  193 | Reward:   -22.90 | Eps: 0.558 | Loss: 4461.9082\n",
      "Ep  194 | Reward:  -241.91 | Eps: 0.557 | Loss: 249996.8750\n",
      "Ep  195 | Reward:   -93.39 | Eps: 0.555 | Loss: 912.9297\n",
      "Ep  196 | Reward:   -79.17 | Eps: 0.553 | Loss: 10080.3477\n",
      "Ep  197 | Reward:   -58.91 | Eps: 0.552 | Loss: 54340.9180\n",
      "Ep  198 | Reward:  -335.50 | Eps: 0.550 | Loss: 12956.3408\n",
      "Ep  199 | Reward:  -160.24 | Eps: 0.548 | Loss: 68241.9609\n",
      "Ep  200 | Reward:   -76.29 | Eps: 0.547 | Loss: 611.7641\n",
      "Ep  201 | Reward:   -47.61 | Eps: 0.545 | Loss: 7957.7451\n",
      "Ep  202 | Reward:  -184.68 | Eps: 0.543 | Loss: 187883.6562\n",
      "Ep  203 | Reward:  -171.30 | Eps: 0.542 | Loss: 39982.3711\n",
      "Ep  204 | Reward:  -185.01 | Eps: 0.540 | Loss: 1960.3600\n",
      "Ep  205 | Reward:   -24.87 | Eps: 0.539 | Loss: 19105.4766\n",
      "Ep  206 | Reward:  -250.72 | Eps: 0.537 | Loss: 5631.5596\n",
      "Ep  207 | Reward:  -161.35 | Eps: 0.535 | Loss: 3681.5247\n",
      "Ep  208 | Reward:  -187.49 | Eps: 0.534 | Loss: 56875.1680\n",
      "Ep  209 | Reward:  -165.43 | Eps: 0.532 | Loss: 98494.5312\n",
      "Ep  210 | Reward:  -146.19 | Eps: 0.530 | Loss: 5498.8994\n",
      "Ep  211 | Reward:  -185.40 | Eps: 0.529 | Loss: 839.7747\n",
      "Ep  212 | Reward:  -153.56 | Eps: 0.527 | Loss: 497721.7812\n",
      "Ep  213 | Reward:  -176.35 | Eps: 0.526 | Loss: 135072.8594\n",
      "Ep  214 | Reward:  -185.35 | Eps: 0.524 | Loss: 382878.0312\n",
      "Ep  215 | Reward:  -340.59 | Eps: 0.523 | Loss: 133624.5469\n",
      "Ep  216 | Reward:  -313.25 | Eps: 0.521 | Loss: 25256.3203\n",
      "Ep  217 | Reward:  -204.66 | Eps: 0.519 | Loss: 12115.3809\n",
      "Ep  218 | Reward:   -97.50 | Eps: 0.518 | Loss: 47352.2500\n",
      "Ep  219 | Reward:   -67.54 | Eps: 0.516 | Loss: 47992.0508\n",
      "Ep  220 | Reward:  -461.95 | Eps: 0.515 | Loss: 73976.2266\n",
      "Ep  221 | Reward:   -69.29 | Eps: 0.513 | Loss: 185803.1406\n",
      "Ep  222 | Reward:  -440.41 | Eps: 0.512 | Loss: 12260.7734\n",
      "Ep  223 | Reward:  -249.48 | Eps: 0.510 | Loss: 109076.4609\n",
      "Ep  224 | Reward:   -90.23 | Eps: 0.509 | Loss: 2335.1340\n",
      "Ep  225 | Reward:  -114.45 | Eps: 0.507 | Loss: 42420.9922\n",
      "Ep  226 | Reward:  -325.57 | Eps: 0.506 | Loss: 4881.7656\n",
      "Ep  227 | Reward:  -327.02 | Eps: 0.504 | Loss: 9159.3555\n",
      "Ep  228 | Reward:   -90.86 | Eps: 0.503 | Loss: 3473.4915\n",
      "Ep  229 | Reward:  -208.62 | Eps: 0.501 | Loss: 15023.6943\n",
      "Ep  230 | Reward:  -116.06 | Eps: 0.500 | Loss: 16891.0254\n",
      "Ep  231 | Reward:  -143.95 | Eps: 0.498 | Loss: 56374.8789\n",
      "Ep  232 | Reward:   -90.36 | Eps: 0.497 | Loss: 17599.7266\n",
      "Ep  233 | Reward:  -103.51 | Eps: 0.495 | Loss: 63626.2656\n",
      "Ep  234 | Reward:  -155.62 | Eps: 0.494 | Loss: 7660.2397\n",
      "Ep  235 | Reward:  -118.73 | Eps: 0.492 | Loss: 11011.6797\n",
      "Ep  236 | Reward:  -143.45 | Eps: 0.491 | Loss: 49526.9570\n",
      "Ep  237 | Reward:   -96.92 | Eps: 0.489 | Loss: 7073.2285\n",
      "Ep  238 | Reward:  -129.66 | Eps: 0.488 | Loss: 24438.6152\n",
      "Ep  239 | Reward:  -113.66 | Eps: 0.486 | Loss: 18745.4727\n",
      "Ep  240 | Reward:  -344.67 | Eps: 0.485 | Loss: 10708.8545\n",
      "Ep  241 | Reward:  -109.24 | Eps: 0.483 | Loss: 17144.2422\n",
      "Ep  242 | Reward:  -107.07 | Eps: 0.482 | Loss: 25946.7363\n",
      "Ep  243 | Reward:  -185.81 | Eps: 0.480 | Loss: 239315.7344\n",
      "Ep  244 | Reward:  -291.70 | Eps: 0.479 | Loss: 333675.0312\n",
      "Ep  245 | Reward:  -167.27 | Eps: 0.478 | Loss: 192823.0000\n",
      "Ep  246 | Reward:   -81.87 | Eps: 0.476 | Loss: 14805.2217\n",
      "Ep  247 | Reward:   -77.37 | Eps: 0.475 | Loss: 43865.7266\n",
      "Ep  248 | Reward:  -103.50 | Eps: 0.473 | Loss: 28327.0273\n",
      "Ep  249 | Reward:   -58.13 | Eps: 0.472 | Loss: 468722.9062\n",
      "Ep  250 | Reward:  -589.39 | Eps: 0.470 | Loss: 2934.4822\n",
      "Ep  251 | Reward:  -272.05 | Eps: 0.469 | Loss: 6585.6772\n",
      "Ep  252 | Reward:  -341.72 | Eps: 0.468 | Loss: 27254.4609\n",
      "Ep  253 | Reward:    -4.35 | Eps: 0.466 | Loss: 68726.0391\n",
      "Ep  254 | Reward:  -369.69 | Eps: 0.465 | Loss: 290453.0000\n",
      "Ep  255 | Reward:  -416.22 | Eps: 0.463 | Loss: 89022.3359\n",
      "Ep  256 | Reward:   -55.99 | Eps: 0.462 | Loss: 1337329.3750\n",
      "Ep  257 | Reward:  -124.92 | Eps: 0.461 | Loss: 18206.8418\n",
      "Ep  258 | Reward:  -117.86 | Eps: 0.459 | Loss: 181604.4219\n",
      "Ep  259 | Reward:  -237.92 | Eps: 0.458 | Loss: 64510.4961\n",
      "Ep  260 | Reward:  -269.24 | Eps: 0.456 | Loss: 40724.3828\n",
      "Ep  261 | Reward:  -554.70 | Eps: 0.455 | Loss: 15987.5898\n",
      "Ep  262 | Reward:   -80.80 | Eps: 0.454 | Loss: 39633.9805\n",
      "Ep  263 | Reward:  -378.97 | Eps: 0.452 | Loss: 15167.8125\n",
      "Ep  264 | Reward:  -295.32 | Eps: 0.451 | Loss: 19408.7188\n",
      "Ep  265 | Reward:  -282.78 | Eps: 0.450 | Loss: 29073.1543\n",
      "Ep  266 | Reward:  -376.12 | Eps: 0.448 | Loss: 57526.0586\n",
      "Ep  267 | Reward:   -97.57 | Eps: 0.447 | Loss: 1699145.0000\n",
      "Ep  268 | Reward:  -185.36 | Eps: 0.446 | Loss: 60059.4492\n",
      "Ep  269 | Reward:  -297.88 | Eps: 0.444 | Loss: 1107482.6250\n",
      "Ep  270 | Reward:  -217.91 | Eps: 0.443 | Loss: 48574.3008\n",
      "Ep  271 | Reward:  -339.71 | Eps: 0.442 | Loss: 24245.3359\n",
      "Ep  272 | Reward:   -97.36 | Eps: 0.440 | Loss: 457261.5625\n",
      "Ep  273 | Reward:   -22.29 | Eps: 0.439 | Loss: 11258.9824\n",
      "Ep  274 | Reward:  -289.71 | Eps: 0.438 | Loss: 14227.3662\n",
      "Ep  275 | Reward:  -136.98 | Eps: 0.436 | Loss: 58996.1367\n",
      "Ep  276 | Reward:  -433.65 | Eps: 0.435 | Loss: 205674.8281\n",
      "Ep  277 | Reward:   -50.53 | Eps: 0.434 | Loss: 88624.7891\n",
      "Ep  278 | Reward:  -190.92 | Eps: 0.432 | Loss: 106320.6953\n",
      "Ep  279 | Reward:  -229.14 | Eps: 0.431 | Loss: 50940.0508\n",
      "Ep  280 | Reward:  -134.83 | Eps: 0.430 | Loss: 25773.8008\n",
      "Ep  281 | Reward:  -240.24 | Eps: 0.429 | Loss: 317478.7812\n",
      "Ep  282 | Reward:  -587.88 | Eps: 0.427 | Loss: 21773.7656\n",
      "Ep  283 | Reward:  -544.33 | Eps: 0.426 | Loss: 555537.5000\n",
      "Ep  284 | Reward:  -434.68 | Eps: 0.425 | Loss: 17708.6152\n",
      "Ep  285 | Reward:  -594.95 | Eps: 0.423 | Loss: 33863.1602\n",
      "Ep  286 | Reward:  -396.01 | Eps: 0.422 | Loss: 196680.5312\n",
      "Ep  287 | Reward:  -621.33 | Eps: 0.421 | Loss: 1968318.2500\n",
      "Ep  288 | Reward:  -409.43 | Eps: 0.420 | Loss: 371339.6250\n",
      "Ep  289 | Reward:  -138.89 | Eps: 0.418 | Loss: 18803.3984\n",
      "Ep  290 | Reward:   -45.29 | Eps: 0.417 | Loss: 75479.7422\n",
      "Ep  291 | Reward:  -302.99 | Eps: 0.416 | Loss: 408056.7500\n",
      "Ep  292 | Reward:  -278.80 | Eps: 0.415 | Loss: 56010.0156\n",
      "Ep  293 | Reward:  -303.83 | Eps: 0.413 | Loss: 110212.0078\n",
      "Ep  294 | Reward:  -269.35 | Eps: 0.412 | Loss: 42706.0469\n",
      "Ep  295 | Reward:  -198.75 | Eps: 0.411 | Loss: 34864.5664\n",
      "Ep  296 | Reward:  -690.90 | Eps: 0.410 | Loss: 281507.6875\n",
      "Ep  297 | Reward:   -97.38 | Eps: 0.408 | Loss: 134056.6406\n",
      "Ep  298 | Reward:  -404.94 | Eps: 0.407 | Loss: 21334.9805\n",
      "Ep  299 | Reward:  -106.24 | Eps: 0.406 | Loss: 24469.5742\n",
      "Ep  300 | Reward:  -319.29 | Eps: 0.405 | Loss: 28778.8516\n",
      "Ep  301 | Reward:  -333.57 | Eps: 0.404 | Loss: 87800.2188\n",
      "Ep  302 | Reward:  -618.27 | Eps: 0.402 | Loss: 43520.7734\n",
      "Ep  303 | Reward:  -342.22 | Eps: 0.401 | Loss: 63206.2461\n",
      "Ep  304 | Reward:  -375.25 | Eps: 0.400 | Loss: 435889.1875\n",
      "Ep  305 | Reward:  -620.98 | Eps: 0.399 | Loss: 24263.6172\n",
      "Ep  306 | Reward:  -611.08 | Eps: 0.398 | Loss: 48799.5195\n",
      "Ep  307 | Reward:  -540.52 | Eps: 0.396 | Loss: 483880.0312\n",
      "Ep  308 | Reward:  -412.36 | Eps: 0.395 | Loss: 52843.8086\n",
      "Ep  309 | Reward:  -334.11 | Eps: 0.394 | Loss: 39048.4375\n",
      "Ep  310 | Reward:  -283.33 | Eps: 0.393 | Loss: 16502.6191\n",
      "Ep  311 | Reward:  -161.11 | Eps: 0.392 | Loss: 20787.8516\n",
      "Ep  312 | Reward:  -308.05 | Eps: 0.390 | Loss: 34788.7656\n",
      "Ep  313 | Reward:   -91.19 | Eps: 0.389 | Loss: 29546.2930\n",
      "Ep  314 | Reward:  -228.87 | Eps: 0.388 | Loss: 52174.0469\n",
      "Ep  315 | Reward:  -245.91 | Eps: 0.387 | Loss: 619726.6250\n",
      "Ep  316 | Reward:  -482.04 | Eps: 0.386 | Loss: 79546.6719\n",
      "Ep  317 | Reward:  -429.70 | Eps: 0.385 | Loss: 54754.8359\n",
      "Ep  318 | Reward:  -491.36 | Eps: 0.383 | Loss: 17004.5430\n",
      "Ep  319 | Reward:   -44.23 | Eps: 0.382 | Loss: 17041.8867\n",
      "Ep  320 | Reward:  -593.42 | Eps: 0.381 | Loss: 84853.1250\n",
      "Ep  321 | Reward:   -58.68 | Eps: 0.380 | Loss: 27000.7539\n",
      "Ep  322 | Reward:  -395.34 | Eps: 0.379 | Loss: 43300.1914\n",
      "Ep  323 | Reward:  -353.85 | Eps: 0.378 | Loss: 41791.3828\n",
      "Ep  324 | Reward:  -431.20 | Eps: 0.377 | Loss: 27192.6758\n",
      "Ep  325 | Reward:  -347.01 | Eps: 0.376 | Loss: 22192.8262\n",
      "Ep  326 | Reward:  -288.13 | Eps: 0.374 | Loss: 253759.8594\n",
      "Ep  327 | Reward:  -457.42 | Eps: 0.373 | Loss: 16596.2383\n",
      "Ep  328 | Reward:  -502.60 | Eps: 0.372 | Loss: 227657.7031\n",
      "Ep  329 | Reward:   -98.07 | Eps: 0.371 | Loss: 40713.6836\n",
      "Ep  330 | Reward:    25.15 | Eps: 0.370 | Loss: 45918.6484\n",
      "Ep  331 | Reward:  -574.98 | Eps: 0.369 | Loss: 41488.6094\n",
      "Ep  332 | Reward:  -338.53 | Eps: 0.368 | Loss: 325363.0312\n",
      "Ep  333 | Reward:  -417.10 | Eps: 0.367 | Loss: 48823.9961\n",
      "Ep  334 | Reward:  -115.02 | Eps: 0.365 | Loss: 19250.4141\n",
      "Ep  335 | Reward:  -123.53 | Eps: 0.364 | Loss: 19252.8223\n",
      "Ep  336 | Reward:  -531.22 | Eps: 0.363 | Loss: 35944.2383\n",
      "Ep  337 | Reward:  -516.68 | Eps: 0.362 | Loss: 23464.2383\n",
      "Ep  338 | Reward:  -439.17 | Eps: 0.361 | Loss: 17532.9629\n",
      "Ep  339 | Reward:  -549.79 | Eps: 0.360 | Loss: 87946.3906\n",
      "Ep  340 | Reward:  -685.72 | Eps: 0.359 | Loss: 13720.4502\n",
      "Ep  341 | Reward:  -300.51 | Eps: 0.358 | Loss: 30843.5742\n",
      "Ep  342 | Reward:  -529.10 | Eps: 0.357 | Loss: 25998.6660\n",
      "Ep  343 | Reward:  -414.36 | Eps: 0.356 | Loss: 12137.1582\n",
      "Ep  344 | Reward:  -414.53 | Eps: 0.355 | Loss: 66823.0391\n",
      "Ep  345 | Reward:   -94.76 | Eps: 0.354 | Loss: 33859.6367\n",
      "Ep  346 | Reward:  -382.95 | Eps: 0.353 | Loss: 40943.7227\n",
      "Ep  347 | Reward:  -372.12 | Eps: 0.351 | Loss: 15627.1934\n",
      "Ep  348 | Reward:   -19.31 | Eps: 0.350 | Loss: 25431.0703\n",
      "Ep  349 | Reward:  -387.06 | Eps: 0.349 | Loss: 35800.7969\n",
      "Ep  350 | Reward:  -640.29 | Eps: 0.348 | Loss: 11311.9629\n",
      "Ep  351 | Reward:  -498.11 | Eps: 0.347 | Loss: 17540.7305\n",
      "Ep  352 | Reward:  -575.16 | Eps: 0.346 | Loss: 41944.7148\n",
      "Ep  353 | Reward:  -482.95 | Eps: 0.345 | Loss: 18731.6953\n",
      "Ep  354 | Reward:  -570.07 | Eps: 0.344 | Loss: 10130.3906\n",
      "Ep  355 | Reward:  -367.07 | Eps: 0.343 | Loss: 46847.8281\n",
      "Ep  356 | Reward:  -328.47 | Eps: 0.342 | Loss: 40530.3672\n",
      "Ep  357 | Reward:  -362.91 | Eps: 0.341 | Loss: 40569.3438\n",
      "Ep  358 | Reward:  -395.63 | Eps: 0.340 | Loss: 33724.0938\n",
      "Ep  359 | Reward:  -391.90 | Eps: 0.339 | Loss: 21231.4746\n",
      "Ep  360 | Reward:  -333.46 | Eps: 0.338 | Loss: 44859.2773\n",
      "Ep  361 | Reward:  -375.64 | Eps: 0.337 | Loss: 24327.8008\n",
      "Ep  362 | Reward:  -323.89 | Eps: 0.336 | Loss: 98099.1719\n",
      "Ep  363 | Reward:  -419.23 | Eps: 0.335 | Loss: 31339.8008\n",
      "Ep  364 | Reward:   -92.95 | Eps: 0.334 | Loss: 33191.6172\n",
      "Ep  365 | Reward:  -355.76 | Eps: 0.333 | Loss: 22344.1836\n",
      "Ep  366 | Reward:  -473.19 | Eps: 0.332 | Loss: 25762.5430\n",
      "Ep  367 | Reward:  -295.54 | Eps: 0.331 | Loss: 36429.1094\n",
      "Ep  368 | Reward:  -275.55 | Eps: 0.330 | Loss: 20669.2930\n",
      "Ep  369 | Reward:  -354.40 | Eps: 0.329 | Loss: 28858.0547\n",
      "Ep  370 | Reward:  -292.23 | Eps: 0.328 | Loss: 26160.6230\n",
      "Ep  371 | Reward:  -382.59 | Eps: 0.327 | Loss: 26968.4199\n",
      "Ep  372 | Reward:  -591.25 | Eps: 0.326 | Loss: 23689.6621\n",
      "Ep  373 | Reward:  -366.61 | Eps: 0.325 | Loss: 37489.7695\n",
      "Ep  374 | Reward:  -296.76 | Eps: 0.324 | Loss: 28327.4727\n",
      "Ep  375 | Reward:  -329.95 | Eps: 0.323 | Loss: 12103.4570\n",
      "Ep  376 | Reward:  -361.15 | Eps: 0.322 | Loss: 45048.8984\n",
      "Ep  377 | Reward:  -410.40 | Eps: 0.321 | Loss: 36563.4414\n",
      "Ep  378 | Reward:  -253.68 | Eps: 0.320 | Loss: 35189.9727\n",
      "Ep  379 | Reward:  -316.83 | Eps: 0.319 | Loss: 41743.5859\n",
      "Ep  380 | Reward:  -243.52 | Eps: 0.318 | Loss: 11754.3262\n",
      "Ep  381 | Reward:  -597.28 | Eps: 0.317 | Loss: 37561.6914\n",
      "Ep  382 | Reward:  -255.75 | Eps: 0.316 | Loss: 24675.1055\n",
      "Ep  383 | Reward:  -323.50 | Eps: 0.315 | Loss: 22614.8652\n",
      "Ep  384 | Reward:  -235.18 | Eps: 0.315 | Loss: 42196.8594\n",
      "Ep  385 | Reward:  -333.13 | Eps: 0.314 | Loss: 29236.9160\n",
      "Ep  386 | Reward:  -237.87 | Eps: 0.313 | Loss: 36054.0625\n",
      "Ep  387 | Reward:  -341.49 | Eps: 0.312 | Loss: 63437.0039\n",
      "Ep  388 | Reward:  -343.65 | Eps: 0.311 | Loss: 44518.6328\n",
      "Ep  389 | Reward:  -256.98 | Eps: 0.310 | Loss: 18318.9805\n",
      "Ep  390 | Reward:  -209.25 | Eps: 0.309 | Loss: 22756.4609\n",
      "Ep  391 | Reward:  -313.05 | Eps: 0.308 | Loss: 27980.2500\n",
      "Ep  392 | Reward:  -175.47 | Eps: 0.307 | Loss: 22760.0020\n",
      "Ep  393 | Reward:  -152.90 | Eps: 0.306 | Loss: 26934.5312\n",
      "Ep  394 | Reward:  -204.07 | Eps: 0.305 | Loss: 15077.9766\n",
      "Ep  395 | Reward:  -285.97 | Eps: 0.304 | Loss: 34980.7383\n",
      "Ep  396 | Reward:  -200.41 | Eps: 0.303 | Loss: 14998.3516\n",
      "Ep  397 | Reward:  -460.61 | Eps: 0.302 | Loss: 21025.1641\n",
      "Ep  398 | Reward:  -205.71 | Eps: 0.302 | Loss: 32992.7656\n",
      "Ep  399 | Reward:  -213.45 | Eps: 0.301 | Loss: 37135.2227\n",
      "Ep  400 | Reward:  -475.07 | Eps: 0.300 | Loss: 2083654.8750\n",
      "Ep  401 | Reward:  -172.16 | Eps: 0.299 | Loss: 65638.3047\n",
      "Ep  402 | Reward:  -215.22 | Eps: 0.298 | Loss: 28468.6074\n",
      "Ep  403 | Reward:  -262.93 | Eps: 0.297 | Loss: 28273.2559\n",
      "Ep  404 | Reward:  -199.23 | Eps: 0.296 | Loss: 35152.8203\n",
      "Ep  405 | Reward:   -55.59 | Eps: 0.295 | Loss: 30995.8574\n",
      "Ep  406 | Reward:  -268.72 | Eps: 0.294 | Loss: 27151.5957\n",
      "Ep  407 | Reward:  -190.85 | Eps: 0.294 | Loss: 29045.1172\n",
      "Ep  408 | Reward:  -305.52 | Eps: 0.293 | Loss: 31095.5176\n",
      "Ep  409 | Reward:  -173.99 | Eps: 0.292 | Loss: 29671.7715\n",
      "Ep  410 | Reward:  -118.89 | Eps: 0.291 | Loss: 111621.8281\n",
      "Ep  411 | Reward:  -231.51 | Eps: 0.290 | Loss: 24626.3203\n",
      "Ep  412 | Reward:  -177.53 | Eps: 0.289 | Loss: 53225.4609\n",
      "Ep  413 | Reward:  -123.56 | Eps: 0.288 | Loss: 17925.3086\n",
      "Ep  414 | Reward:  -367.60 | Eps: 0.287 | Loss: 47967.2148\n",
      "Ep  415 | Reward:  -146.84 | Eps: 0.287 | Loss: 35190.1641\n",
      "Ep  416 | Reward:  -221.73 | Eps: 0.286 | Loss: 28834.2617\n",
      "Ep  417 | Reward:  -350.92 | Eps: 0.285 | Loss: 24251.4121\n",
      "Ep  418 | Reward:  -361.81 | Eps: 0.284 | Loss: 222256.4531\n",
      "Ep  419 | Reward:  -151.36 | Eps: 0.283 | Loss: 224605.4375\n",
      "Ep  420 | Reward:  -148.24 | Eps: 0.282 | Loss: 21961.1484\n",
      "Ep  421 | Reward:  -195.30 | Eps: 0.281 | Loss: 26034.2969\n",
      "Ep  422 | Reward:   -91.70 | Eps: 0.281 | Loss: 16819.4219\n",
      "Ep  423 | Reward:  -181.46 | Eps: 0.280 | Loss: 24894.1602\n",
      "Ep  424 | Reward:  -274.81 | Eps: 0.279 | Loss: 27565.2168\n",
      "Ep  425 | Reward:  -236.33 | Eps: 0.278 | Loss: 20089.7422\n",
      "Ep  426 | Reward:  -152.70 | Eps: 0.277 | Loss: 46064.8398\n",
      "Ep  427 | Reward:  -183.67 | Eps: 0.276 | Loss: 12949.0273\n",
      "Ep  428 | Reward:    63.29 | Eps: 0.276 | Loss: 48715.4180\n",
      "Ep  429 | Reward:  -168.72 | Eps: 0.275 | Loss: 26792.6074\n",
      "Ep  430 | Reward:  -145.88 | Eps: 0.274 | Loss: 30881.2109\n",
      "Ep  431 | Reward:  -232.09 | Eps: 0.273 | Loss: 23719.4199\n",
      "Ep  432 | Reward:  -201.20 | Eps: 0.272 | Loss: 79681.7969\n",
      "Ep  433 | Reward:  -193.78 | Eps: 0.271 | Loss: 67498.7969\n",
      "Ep  434 | Reward:  -164.91 | Eps: 0.271 | Loss: 18999.3750\n",
      "Ep  435 | Reward:  -325.38 | Eps: 0.270 | Loss: 103434.5547\n",
      "Ep  436 | Reward:  -148.13 | Eps: 0.269 | Loss: 19425.6797\n",
      "Ep  437 | Reward:  -284.85 | Eps: 0.268 | Loss: 85793.9297\n",
      "Ep  438 | Reward:  -185.53 | Eps: 0.267 | Loss: 96512.8359\n",
      "Ep  439 | Reward:  -109.35 | Eps: 0.267 | Loss: 55665.7305\n",
      "Ep  440 | Reward:  -181.55 | Eps: 0.266 | Loss: 29879.6836\n",
      "Ep  441 | Reward:  -147.52 | Eps: 0.265 | Loss: 119607.9297\n",
      "Ep  442 | Reward:  -275.16 | Eps: 0.264 | Loss: 43098.1055\n",
      "Ep  443 | Reward:  -103.94 | Eps: 0.263 | Loss: 49616.1562\n",
      "Ep  444 | Reward:  -187.63 | Eps: 0.263 | Loss: 30431.2383\n",
      "Ep  445 | Reward:  -130.31 | Eps: 0.262 | Loss: 16391.3262\n",
      "Ep  446 | Reward:  -152.23 | Eps: 0.261 | Loss: 38600.9648\n",
      "Ep  447 | Reward:   -84.78 | Eps: 0.260 | Loss: 49775.9219\n",
      "Ep  448 | Reward:  -179.57 | Eps: 0.259 | Loss: 19299.1895\n",
      "Ep  449 | Reward:  -138.31 | Eps: 0.259 | Loss: 93890.8203\n",
      "Ep  450 | Reward:  -160.94 | Eps: 0.258 | Loss: 21544.8418\n",
      "Ep  451 | Reward:  -195.28 | Eps: 0.257 | Loss: 19838.8887\n",
      "Ep  452 | Reward:  -267.96 | Eps: 0.256 | Loss: 25833.7441\n",
      "Ep  453 | Reward:  -248.65 | Eps: 0.256 | Loss: 62593.7539\n",
      "Ep  454 | Reward:  -224.54 | Eps: 0.255 | Loss: 28410.9531\n",
      "Ep  455 | Reward:  -320.05 | Eps: 0.254 | Loss: 18172.3359\n",
      "Ep  456 | Reward:  -307.66 | Eps: 0.253 | Loss: 29613.6309\n",
      "Ep  457 | Reward:  -137.17 | Eps: 0.253 | Loss: 259956.2812\n",
      "Ep  458 | Reward:  -151.69 | Eps: 0.252 | Loss: 23153.4062\n",
      "Ep  459 | Reward:   -84.53 | Eps: 0.251 | Loss: 22641.2754\n",
      "Ep  460 | Reward:  -255.13 | Eps: 0.250 | Loss: 23019.3418\n",
      "Ep  461 | Reward:  -192.24 | Eps: 0.250 | Loss: 23980.0215\n",
      "Ep  462 | Reward:  -143.22 | Eps: 0.249 | Loss: 17347.8672\n",
      "Ep  463 | Reward:  -233.13 | Eps: 0.248 | Loss: 15071.7285\n",
      "Ep  464 | Reward:  -132.31 | Eps: 0.247 | Loss: 22864.2988\n",
      "Ep  465 | Reward:  -233.65 | Eps: 0.247 | Loss: 17389.5391\n",
      "Ep  466 | Reward:  -151.36 | Eps: 0.246 | Loss: 30550.6055\n",
      "Ep  467 | Reward:  -139.39 | Eps: 0.245 | Loss: 21439.3887\n",
      "Ep  468 | Reward:  -113.72 | Eps: 0.244 | Loss: 27746.7812\n",
      "Ep  469 | Reward:  -214.01 | Eps: 0.244 | Loss: 84384.6406\n",
      "Ep  470 | Reward:  -266.07 | Eps: 0.243 | Loss: 151401.8594\n",
      "Ep  471 | Reward:  -176.81 | Eps: 0.242 | Loss: 25106.2285\n",
      "Ep  472 | Reward:  -230.13 | Eps: 0.241 | Loss: 26974.4883\n",
      "Ep  473 | Reward:   -99.03 | Eps: 0.241 | Loss: 16610.6133\n",
      "Ep  474 | Reward:  -276.14 | Eps: 0.240 | Loss: 69802.2734\n",
      "Ep  475 | Reward:  -362.69 | Eps: 0.239 | Loss: 33773.1172\n",
      "Ep  476 | Reward:  -187.47 | Eps: 0.239 | Loss: 39245.2422\n",
      "Ep  477 | Reward:  -306.62 | Eps: 0.238 | Loss: 32380.8477\n",
      "Ep  478 | Reward:  -207.90 | Eps: 0.237 | Loss: 25293.1758\n",
      "Ep  479 | Reward:  -156.22 | Eps: 0.236 | Loss: 36895.3359\n",
      "Ep  480 | Reward:  -106.56 | Eps: 0.236 | Loss: 264670.0625\n",
      "Ep  481 | Reward:  -238.45 | Eps: 0.235 | Loss: 31002.6680\n",
      "Ep  482 | Reward:  -189.20 | Eps: 0.234 | Loss: 12997.6611\n",
      "Ep  483 | Reward:  -200.27 | Eps: 0.234 | Loss: 24633.6484\n",
      "Ep  484 | Reward:   -95.78 | Eps: 0.233 | Loss: 27400.0703\n",
      "Ep  485 | Reward:  -257.98 | Eps: 0.232 | Loss: 18112.8105\n",
      "Ep  486 | Reward:  -289.60 | Eps: 0.231 | Loss: 18201.6699\n",
      "Ep  487 | Reward:  -280.44 | Eps: 0.231 | Loss: 31753.7910\n",
      "Ep  488 | Reward:  -145.93 | Eps: 0.230 | Loss: 32004.3906\n",
      "Ep  489 | Reward:  -145.70 | Eps: 0.229 | Loss: 68557.6094\n",
      "Ep  490 | Reward:  -209.14 | Eps: 0.229 | Loss: 40808.7656\n",
      "Ep  491 | Reward:  -168.75 | Eps: 0.228 | Loss: 32056.1191\n",
      "Ep  492 | Reward:  -192.79 | Eps: 0.227 | Loss: 37109.6836\n",
      "Ep  493 | Reward:  -151.90 | Eps: 0.227 | Loss: 51776.4766\n",
      "Ep  494 | Reward:  -171.98 | Eps: 0.226 | Loss: 16163.4844\n",
      "Ep  495 | Reward:  -138.47 | Eps: 0.225 | Loss: 25266.6777\n",
      "Ep  496 | Reward:  -187.26 | Eps: 0.225 | Loss: 14142.9082\n",
      "Ep  497 | Reward:  -124.99 | Eps: 0.224 | Loss: 18964.0605\n",
      "Ep  498 | Reward:  -216.87 | Eps: 0.223 | Loss: 15506.0625\n",
      "Ep  499 | Reward:  -166.11 | Eps: 0.223 | Loss: 22876.7461\n",
      "Ep  500 | Reward:  -203.54 | Eps: 0.222 | Loss: 20394.6914\n",
      "Ep  501 | Reward:  -188.95 | Eps: 0.221 | Loss: 146073.0156\n",
      "Ep  502 | Reward:  -111.74 | Eps: 0.221 | Loss: 17584.2188\n",
      "Ep  503 | Reward:  -165.17 | Eps: 0.220 | Loss: 21438.2754\n",
      "Ep  504 | Reward:  -139.96 | Eps: 0.219 | Loss: 154772.4688\n",
      "Ep  505 | Reward:   -82.14 | Eps: 0.219 | Loss: 20892.1484\n",
      "Ep  506 | Reward:  -201.04 | Eps: 0.218 | Loss: 30101.1035\n",
      "Ep  507 | Reward:  -129.01 | Eps: 0.217 | Loss: 19060.8965\n",
      "Ep  508 | Reward:  -195.64 | Eps: 0.217 | Loss: 84545.2266\n",
      "Ep  509 | Reward:  -198.93 | Eps: 0.216 | Loss: 42943.7227\n",
      "Ep  510 | Reward:  -169.92 | Eps: 0.215 | Loss: 37888.5430\n",
      "Ep  511 | Reward:  -216.26 | Eps: 0.215 | Loss: 21211.9219\n",
      "Ep  512 | Reward:  -194.50 | Eps: 0.214 | Loss: 20404.3594\n",
      "Ep  513 | Reward:  -152.50 | Eps: 0.213 | Loss: 24962.3105\n",
      "Ep  514 | Reward:  -128.10 | Eps: 0.213 | Loss: 14078.7402\n",
      "Ep  515 | Reward:  -153.36 | Eps: 0.212 | Loss: 122563.5000\n",
      "Ep  516 | Reward:   -71.68 | Eps: 0.212 | Loss: 502155.4062\n",
      "Ep  517 | Reward:  -143.12 | Eps: 0.211 | Loss: 44235.9023\n",
      "Ep  518 | Reward:  -103.31 | Eps: 0.210 | Loss: 34044.9922\n",
      "Ep  519 | Reward:  -126.93 | Eps: 0.210 | Loss: 427442.4688\n",
      "Ep  520 | Reward:  -213.72 | Eps: 0.209 | Loss: 21317.2266\n",
      "Ep  521 | Reward:  -148.16 | Eps: 0.208 | Loss: 25337.2852\n",
      "Ep  522 | Reward:  -166.32 | Eps: 0.208 | Loss: 946913.4375\n",
      "Ep  523 | Reward:  -202.18 | Eps: 0.207 | Loss: 589548.1875\n",
      "Ep  524 | Reward:  -151.58 | Eps: 0.207 | Loss: 31746.9453\n",
      "Ep  525 | Reward:  -193.53 | Eps: 0.206 | Loss: 26271.2227\n",
      "Ep  526 | Reward:  -163.66 | Eps: 0.205 | Loss: 160232.1094\n",
      "Ep  527 | Reward:  -154.55 | Eps: 0.205 | Loss: 25869.3984\n",
      "Ep  528 | Reward:  -184.12 | Eps: 0.204 | Loss: 28233.9492\n",
      "Ep  529 | Reward:  -180.00 | Eps: 0.203 | Loss: 558451.0625\n",
      "Ep  530 | Reward:  -176.19 | Eps: 0.203 | Loss: 14341.6963\n",
      "Ep  531 | Reward:  -169.63 | Eps: 0.202 | Loss: 27705.3223\n",
      "Ep  532 | Reward:   -89.53 | Eps: 0.202 | Loss: 31600.4141\n",
      "Ep  533 | Reward:  -113.50 | Eps: 0.201 | Loss: 20415.6641\n",
      "Ep  534 | Reward:  -181.86 | Eps: 0.200 | Loss: 43649.5820\n",
      "Ep  535 | Reward:  -121.69 | Eps: 0.200 | Loss: 29715.3086\n",
      "Ep  536 | Reward:  -165.96 | Eps: 0.199 | Loss: 23868.5898\n",
      "Ep  537 | Reward:  -190.82 | Eps: 0.199 | Loss: 17009.4336\n",
      "Ep  538 | Reward:  -142.51 | Eps: 0.198 | Loss: 43270.8672\n",
      "Ep  539 | Reward:  -180.48 | Eps: 0.197 | Loss: 22099.6270\n",
      "Ep  540 | Reward:  -132.81 | Eps: 0.197 | Loss: 601087.0625\n",
      "Ep  541 | Reward:  -179.79 | Eps: 0.196 | Loss: 25455.8438\n",
      "Ep  542 | Reward:  -199.07 | Eps: 0.196 | Loss: 25694.9805\n",
      "Ep  543 | Reward:  -182.11 | Eps: 0.195 | Loss: 116602.2891\n",
      "Ep  544 | Reward:   -97.37 | Eps: 0.194 | Loss: 22790.1875\n",
      "Ep  545 | Reward:  -165.50 | Eps: 0.194 | Loss: 24642.2617\n",
      "Ep  546 | Reward:  -170.91 | Eps: 0.193 | Loss: 27051.0684\n",
      "Ep  547 | Reward:  -173.43 | Eps: 0.193 | Loss: 25875.3789\n",
      "Ep  548 | Reward:  -160.48 | Eps: 0.192 | Loss: 26754.8086\n",
      "Ep  549 | Reward:  -240.46 | Eps: 0.192 | Loss: 14610.8457\n",
      "Ep  550 | Reward:  -154.03 | Eps: 0.191 | Loss: 299157.8750\n",
      "Ep  551 | Reward:  -155.93 | Eps: 0.190 | Loss: 19633.0156\n",
      "Ep  552 | Reward:  -135.42 | Eps: 0.190 | Loss: 46317.1016\n",
      "Ep  553 | Reward:  -127.34 | Eps: 0.189 | Loss: 26648.5996\n",
      "Ep  554 | Reward:  -121.85 | Eps: 0.189 | Loss: 28939.4785\n",
      "Ep  555 | Reward:  -134.87 | Eps: 0.188 | Loss: 479909.6875\n",
      "Ep  556 | Reward:   -63.46 | Eps: 0.188 | Loss: 28200.6582\n",
      "Ep  557 | Reward:  -187.59 | Eps: 0.187 | Loss: 273311.0000\n",
      "Ep  558 | Reward:  -160.18 | Eps: 0.186 | Loss: 21807.7383\n",
      "Ep  559 | Reward:  -131.89 | Eps: 0.186 | Loss: 40347.9727\n",
      "Ep  560 | Reward:  -362.06 | Eps: 0.185 | Loss: 39516.4492\n",
      "Ep  561 | Reward:  -131.28 | Eps: 0.185 | Loss: 21289.7168\n",
      "Ep  562 | Reward:  -152.26 | Eps: 0.184 | Loss: 18658.6973\n",
      "Ep  563 | Reward:  -179.13 | Eps: 0.184 | Loss: 195769.2812\n",
      "Ep  564 | Reward:  -199.24 | Eps: 0.183 | Loss: 265150.5938\n",
      "Ep  565 | Reward:  -188.92 | Eps: 0.183 | Loss: 25303.5664\n",
      "Ep  566 | Reward:  -153.56 | Eps: 0.182 | Loss: 33204.5195\n",
      "Ep  567 | Reward:   -83.64 | Eps: 0.181 | Loss: 40359.2227\n",
      "Ep  568 | Reward:   -92.31 | Eps: 0.181 | Loss: 130617.6016\n",
      "Ep  569 | Reward:  -135.30 | Eps: 0.180 | Loss: 33643.6836\n",
      "Ep  570 | Reward:  -137.53 | Eps: 0.180 | Loss: 16944.0059\n",
      "Ep  571 | Reward:  -194.05 | Eps: 0.179 | Loss: 17279.3027\n",
      "Ep  572 | Reward:  -236.06 | Eps: 0.179 | Loss: 38608.1055\n",
      "Ep  573 | Reward:  -151.16 | Eps: 0.178 | Loss: 16727.8008\n",
      "Ep  574 | Reward:  -175.81 | Eps: 0.178 | Loss: 21666.9707\n",
      "Ep  575 | Reward:  -164.17 | Eps: 0.177 | Loss: 25577.3984\n",
      "Ep  576 | Reward:  -155.05 | Eps: 0.177 | Loss: 26018.1953\n",
      "Ep  577 | Reward:  -132.60 | Eps: 0.176 | Loss: 10735.3311\n",
      "Ep  578 | Reward:  -166.76 | Eps: 0.176 | Loss: 13378.5479\n",
      "Ep  579 | Reward:  -236.83 | Eps: 0.175 | Loss: 11632.9463\n",
      "Ep  580 | Reward:  -152.61 | Eps: 0.175 | Loss: 23112.4219\n",
      "Ep  581 | Reward:  -148.02 | Eps: 0.174 | Loss: 173977.4219\n",
      "Ep  582 | Reward:  -237.27 | Eps: 0.173 | Loss: 12563.0078\n",
      "Ep  583 | Reward:   -93.97 | Eps: 0.173 | Loss: 435575.9375\n",
      "Ep  584 | Reward:  -127.40 | Eps: 0.172 | Loss: 10540.9883\n",
      "Ep  585 | Reward:  -142.85 | Eps: 0.172 | Loss: 11040.4385\n",
      "Ep  586 | Reward:  -144.26 | Eps: 0.171 | Loss: 26713.0234\n",
      "Ep  587 | Reward:  -102.09 | Eps: 0.171 | Loss: 14079.5332\n",
      "Ep  588 | Reward:   -98.44 | Eps: 0.170 | Loss: 17738.6816\n",
      "Ep  589 | Reward:   -60.58 | Eps: 0.170 | Loss: 9316.2500\n",
      "Ep  590 | Reward:  -104.48 | Eps: 0.169 | Loss: 7468.1455\n",
      "Ep  591 | Reward:  -128.45 | Eps: 0.169 | Loss: 24016.8672\n",
      "Ep  592 | Reward:   -57.50 | Eps: 0.168 | Loss: 9170.7812\n",
      "Ep  593 | Reward:  -782.37 | Eps: 0.168 | Loss: 35087.7695\n",
      "Ep  594 | Reward:   -49.95 | Eps: 0.167 | Loss: 17834.7637\n",
      "Ep  595 | Reward:  -126.20 | Eps: 0.167 | Loss: 5664.7046\n",
      "Ep  596 | Reward:   -72.07 | Eps: 0.166 | Loss: 65931.7656\n",
      "Ep  597 | Reward:  -111.34 | Eps: 0.166 | Loss: 29981.1289\n",
      "Ep  598 | Reward:   -87.36 | Eps: 0.165 | Loss: 424800.1250\n",
      "Ep  599 | Reward:  -125.69 | Eps: 0.165 | Loss: 6955.3599\n",
      "Ep  600 | Reward:  -482.96 | Eps: 0.164 | Loss: 12899.1318\n",
      "Ep  601 | Reward:   -63.98 | Eps: 0.164 | Loss: 24955.5742\n",
      "Ep  602 | Reward:  -189.44 | Eps: 0.163 | Loss: 131667.2031\n",
      "Ep  603 | Reward:   -32.23 | Eps: 0.163 | Loss: 8363.4102\n",
      "Ep  604 | Reward:  -131.50 | Eps: 0.162 | Loss: 4337.5610\n",
      "Ep  605 | Reward:   -37.44 | Eps: 0.162 | Loss: 5802.1548\n",
      "Ep  606 | Reward:  -212.12 | Eps: 0.161 | Loss: 7351.8477\n",
      "Ep  607 | Reward:   -30.55 | Eps: 0.161 | Loss: 24416.7520\n",
      "Ep  608 | Reward:   -97.57 | Eps: 0.160 | Loss: 1434449.0000\n",
      "Ep  609 | Reward:   -48.66 | Eps: 0.160 | Loss: 14919.6885\n",
      "Ep  610 | Reward:   -98.30 | Eps: 0.159 | Loss: 5157.9209\n",
      "Ep  611 | Reward:   -55.10 | Eps: 0.159 | Loss: 6000.6699\n",
      "Ep  612 | Reward:   -28.51 | Eps: 0.159 | Loss: 1386189.6250\n",
      "Ep  613 | Reward:  -601.39 | Eps: 0.158 | Loss: 4996.0967\n",
      "Ep  614 | Reward: -1103.23 | Eps: 0.158 | Loss: 14847.0605\n",
      "Ep  615 | Reward:  -171.18 | Eps: 0.157 | Loss: 4805.4414\n",
      "Ep  616 | Reward:  -190.02 | Eps: 0.157 | Loss: 6145.9946\n",
      "Ep  617 | Reward:  -291.79 | Eps: 0.156 | Loss: 13257.5234\n",
      "Ep  618 | Reward:  -218.82 | Eps: 0.156 | Loss: 1386714.3750\n",
      "Ep  619 | Reward:   -93.70 | Eps: 0.155 | Loss: 6906.8789\n",
      "Ep  620 | Reward:  -234.21 | Eps: 0.155 | Loss: 5345.5913\n",
      "Ep  621 | Reward:  -227.02 | Eps: 0.154 | Loss: 12018.3213\n",
      "Ep  622 | Reward:  -315.16 | Eps: 0.154 | Loss: 8487.8691\n",
      "Ep  623 | Reward:  -545.73 | Eps: 0.153 | Loss: 7853.2822\n",
      "Ep  624 | Reward:  -218.37 | Eps: 0.153 | Loss: 9438.2744\n",
      "Ep  625 | Reward:  -123.57 | Eps: 0.152 | Loss: 6889.3008\n",
      "Ep  626 | Reward:  -252.65 | Eps: 0.152 | Loss: 5587.8247\n",
      "Ep  627 | Reward:  -219.96 | Eps: 0.152 | Loss: 23108.6016\n",
      "Ep  628 | Reward:   -47.23 | Eps: 0.151 | Loss: 8033.6836\n",
      "Ep  629 | Reward:  -403.67 | Eps: 0.151 | Loss: 5348.2036\n",
      "Ep  630 | Reward:  -275.57 | Eps: 0.150 | Loss: 338162.3438\n",
      "Ep  631 | Reward:  -261.82 | Eps: 0.150 | Loss: 1139673.0000\n",
      "Ep  632 | Reward:  -185.90 | Eps: 0.149 | Loss: 8666.9688\n",
      "Ep  633 | Reward:  -218.73 | Eps: 0.149 | Loss: 1287087.8750\n",
      "Ep  634 | Reward:  -266.22 | Eps: 0.148 | Loss: 8793.4082\n",
      "Ep  635 | Reward:   -83.89 | Eps: 0.148 | Loss: 18526.5039\n",
      "Ep  636 | Reward:  -343.57 | Eps: 0.148 | Loss: 7110.3389\n",
      "Ep  637 | Reward:  -324.19 | Eps: 0.147 | Loss: 12288.1543\n",
      "Ep  638 | Reward:  -233.72 | Eps: 0.147 | Loss: 7425.2573\n",
      "Ep  639 | Reward:  -346.77 | Eps: 0.146 | Loss: 5273.8281\n",
      "Ep  640 | Reward:  -278.31 | Eps: 0.146 | Loss: 7815.3950\n",
      "Ep  641 | Reward:  -188.15 | Eps: 0.145 | Loss: 5648.4541\n",
      "Ep  642 | Reward:  -221.29 | Eps: 0.145 | Loss: 7754.5093\n",
      "Ep  643 | Reward:  -411.58 | Eps: 0.144 | Loss: 8055.0405\n",
      "Ep  644 | Reward:  -258.21 | Eps: 0.144 | Loss: 23751.6055\n",
      "Ep  645 | Reward:  -176.05 | Eps: 0.144 | Loss: 20198.0195\n",
      "Ep  646 | Reward:  -275.94 | Eps: 0.143 | Loss: 20944.9609\n",
      "Ep  647 | Reward:  -277.62 | Eps: 0.143 | Loss: 8856.2979\n",
      "Ep  648 | Reward:  -198.11 | Eps: 0.142 | Loss: 10028.7441\n",
      "Ep  649 | Reward:  -248.61 | Eps: 0.142 | Loss: 5945.7056\n",
      "Ep  650 | Reward:  -291.82 | Eps: 0.141 | Loss: 23140.7324\n",
      "Ep  651 | Reward:  -239.74 | Eps: 0.141 | Loss: 11398.6309\n",
      "Ep  652 | Reward:  -452.61 | Eps: 0.141 | Loss: 67831.9609\n",
      "Ep  653 | Reward:  -264.35 | Eps: 0.140 | Loss: 15022.3145\n",
      "Ep  654 | Reward:  -313.99 | Eps: 0.140 | Loss: 18503.0781\n",
      "Ep  655 | Reward:  -221.25 | Eps: 0.139 | Loss: 10237.6465\n",
      "Ep  656 | Reward:  -148.96 | Eps: 0.139 | Loss: 8380.3516\n",
      "Ep  657 | Reward:  -320.39 | Eps: 0.138 | Loss: 51392.3711\n",
      "Ep  658 | Reward:  -229.45 | Eps: 0.138 | Loss: 15389.6836\n",
      "Ep  659 | Reward:  -157.97 | Eps: 0.138 | Loss: 7317.0156\n",
      "Ep  660 | Reward:  -398.02 | Eps: 0.137 | Loss: 9553.0547\n",
      "Ep  661 | Reward:  -427.99 | Eps: 0.137 | Loss: 7093.0469\n",
      "Ep  662 | Reward:  -282.10 | Eps: 0.136 | Loss: 5660.0273\n",
      "Ep  663 | Reward:  -347.73 | Eps: 0.136 | Loss: 9535.0117\n",
      "Ep  664 | Reward:  -222.75 | Eps: 0.136 | Loss: 7882.5840\n",
      "Ep  665 | Reward:  -271.80 | Eps: 0.135 | Loss: 8557.2100\n",
      "Ep  666 | Reward:  -240.87 | Eps: 0.135 | Loss: 9277.7178\n",
      "Ep  667 | Reward:  -299.42 | Eps: 0.134 | Loss: 7173.0347\n",
      "Ep  668 | Reward:  -257.93 | Eps: 0.134 | Loss: 8943.9219\n",
      "Ep  669 | Reward:  -270.40 | Eps: 0.134 | Loss: 9138.7959\n",
      "Ep  670 | Reward:  -208.02 | Eps: 0.133 | Loss: 8102.7285\n",
      "Ep  671 | Reward:  -216.93 | Eps: 0.133 | Loss: 5308.3345\n",
      "Ep  672 | Reward:  -224.97 | Eps: 0.132 | Loss: 13543.1367\n",
      "Ep  673 | Reward:  -195.37 | Eps: 0.132 | Loss: 88183.9453\n",
      "Ep  674 | Reward:  -350.62 | Eps: 0.132 | Loss: 131791.1562\n",
      "Ep  675 | Reward:  -229.03 | Eps: 0.131 | Loss: 6517.8140\n",
      "Ep  676 | Reward:  -312.85 | Eps: 0.131 | Loss: 73507.9844\n",
      "Ep  677 | Reward:  -318.51 | Eps: 0.130 | Loss: 29259.4062\n",
      "Ep  678 | Reward:  -253.21 | Eps: 0.130 | Loss: 7813.9653\n",
      "Ep  679 | Reward:  -295.17 | Eps: 0.130 | Loss: 32577.0957\n",
      "Ep  680 | Reward:  -257.96 | Eps: 0.129 | Loss: 7982.3574\n",
      "Ep  681 | Reward:  -231.24 | Eps: 0.129 | Loss: 3725.3420\n",
      "Ep  682 | Reward:  -293.97 | Eps: 0.128 | Loss: 4038.2505\n",
      "Ep  683 | Reward:  -438.97 | Eps: 0.128 | Loss: 50312.4062\n",
      "Ep  684 | Reward:  -276.60 | Eps: 0.128 | Loss: 17847.1250\n",
      "Ep  685 | Reward:  -216.04 | Eps: 0.127 | Loss: 6415.0063\n",
      "Ep  686 | Reward:  -367.00 | Eps: 0.127 | Loss: 4651.1440\n",
      "Ep  687 | Reward:  -331.76 | Eps: 0.127 | Loss: 19588.7324\n",
      "Ep  688 | Reward:   -51.93 | Eps: 0.126 | Loss: 16234.6006\n",
      "Ep  689 | Reward:  -250.62 | Eps: 0.126 | Loss: 5678.1069\n",
      "Ep  690 | Reward:  -687.13 | Eps: 0.125 | Loss: 15351.8271\n",
      "Ep  691 | Reward:  -308.67 | Eps: 0.125 | Loss: 39354.8359\n",
      "Ep  692 | Reward:  -569.10 | Eps: 0.125 | Loss: 6936.9395\n",
      "Ep  693 | Reward:  -680.11 | Eps: 0.124 | Loss: 14953.5791\n",
      "Ep  694 | Reward:  -227.95 | Eps: 0.124 | Loss: 49288.0000\n",
      "Ep  695 | Reward:  -249.42 | Eps: 0.124 | Loss: 8525.1035\n",
      "Ep  696 | Reward:  -656.19 | Eps: 0.123 | Loss: 157187.9219\n",
      "Ep  697 | Reward: -1113.66 | Eps: 0.123 | Loss: 9846.8779\n",
      "Ep  698 | Reward:  -241.29 | Eps: 0.122 | Loss: 22815.5820\n",
      "Ep  699 | Reward:  -286.70 | Eps: 0.122 | Loss: 7539689.0000\n",
      "Ep  700 | Reward:  -284.02 | Eps: 0.122 | Loss: 9646.3574\n",
      "Ep  701 | Reward:  -186.95 | Eps: 0.121 | Loss: 11715.2539\n",
      "Ep  702 | Reward:  -895.72 | Eps: 0.121 | Loss: 23674.9375\n",
      "Ep  703 | Reward:  -596.67 | Eps: 0.121 | Loss: 23834.3086\n",
      "Ep  704 | Reward:   -53.41 | Eps: 0.120 | Loss: 100283.0156\n",
      "Ep  705 | Reward:   -62.24 | Eps: 0.120 | Loss: 8888.9902\n",
      "Ep  706 | Reward:  -320.26 | Eps: 0.120 | Loss: 9747.2715\n",
      "Ep  707 | Reward:  -495.66 | Eps: 0.119 | Loss: 20257.3945\n",
      "Ep  708 | Reward:  -646.89 | Eps: 0.119 | Loss: 11728.2578\n",
      "Ep  709 | Reward:  -237.87 | Eps: 0.118 | Loss: 522119.4375\n",
      "Ep  710 | Reward:  -257.81 | Eps: 0.118 | Loss: 27545.4961\n",
      "Ep  711 | Reward:  -510.52 | Eps: 0.118 | Loss: 30391.2520\n",
      "Ep  712 | Reward:   -27.41 | Eps: 0.117 | Loss: 19380.0215\n",
      "Ep  713 | Reward:  -271.85 | Eps: 0.117 | Loss: 16098.1826\n",
      "Ep  714 | Reward:  -255.59 | Eps: 0.117 | Loss: 76776.3203\n",
      "Ep  715 | Reward:  -254.78 | Eps: 0.116 | Loss: 14923.6357\n",
      "Ep  716 | Reward:  -231.89 | Eps: 0.116 | Loss: 10653.0742\n",
      "Ep  717 | Reward:  -547.52 | Eps: 0.116 | Loss: 9286.1123\n",
      "Ep  718 | Reward:  -273.19 | Eps: 0.115 | Loss: 23434.6289\n",
      "Ep  719 | Reward:  -549.37 | Eps: 0.115 | Loss: 10102.4307\n",
      "Ep  720 | Reward:  -410.06 | Eps: 0.115 | Loss: 68615.7969\n",
      "Ep  721 | Reward:  -511.00 | Eps: 0.114 | Loss: 772803.5000\n",
      "Ep  722 | Reward:  -987.34 | Eps: 0.114 | Loss: 51780.5742\n",
      "Ep  723 | Reward:  -554.26 | Eps: 0.114 | Loss: 8519.6797\n",
      "Ep  724 | Reward:  -925.23 | Eps: 0.113 | Loss: 18787.5469\n",
      "Ep  725 | Reward:  -257.37 | Eps: 0.113 | Loss: 11219.5205\n",
      "Ep  726 | Reward:  -227.83 | Eps: 0.113 | Loss: 17978.6855\n",
      "Ep  727 | Reward:  -635.00 | Eps: 0.112 | Loss: 30985.2695\n",
      "Ep  728 | Reward:  -193.35 | Eps: 0.112 | Loss: 15650.6396\n",
      "Ep  729 | Reward:  -484.01 | Eps: 0.112 | Loss: 21059.2539\n",
      "Ep  730 | Reward:  -422.94 | Eps: 0.111 | Loss: 31399.2520\n",
      "Ep  731 | Reward:  -604.31 | Eps: 0.111 | Loss: 9873.8389\n",
      "Ep  732 | Reward:  -429.50 | Eps: 0.111 | Loss: 17045.8906\n",
      "Ep  733 | Reward:  -524.56 | Eps: 0.110 | Loss: 19231.7227\n",
      "Ep  734 | Reward:  -269.71 | Eps: 0.110 | Loss: 92827.7031\n",
      "Ep  735 | Reward:  -325.07 | Eps: 0.110 | Loss: 10224.3047\n",
      "Ep  736 | Reward:  -751.28 | Eps: 0.109 | Loss: 31997.3711\n",
      "Ep  737 | Reward:  -441.64 | Eps: 0.109 | Loss: 9396.2031\n",
      "Ep  738 | Reward:  -571.55 | Eps: 0.109 | Loss: 632453.7500\n",
      "Ep  739 | Reward:  -255.56 | Eps: 0.108 | Loss: 76475.0547\n",
      "Ep  740 | Reward:  -376.11 | Eps: 0.108 | Loss: 24110.1660\n",
      "Ep  741 | Reward:  -559.93 | Eps: 0.108 | Loss: 367758.4688\n",
      "Ep  742 | Reward:  -892.12 | Eps: 0.107 | Loss: 14544.7275\n",
      "Ep  743 | Reward:  -570.46 | Eps: 0.107 | Loss: 18828.3398\n",
      "Ep  744 | Reward:  -203.03 | Eps: 0.107 | Loss: 119926.5781\n",
      "Ep  745 | Reward:  -383.14 | Eps: 0.106 | Loss: 35440.3281\n",
      "Ep  746 | Reward:  -558.52 | Eps: 0.106 | Loss: 8126.8408\n",
      "Ep  747 | Reward:  -641.99 | Eps: 0.106 | Loss: 14106.9219\n",
      "Ep  748 | Reward:  -432.63 | Eps: 0.105 | Loss: 20970.1484\n",
      "Ep  749 | Reward:  -596.35 | Eps: 0.105 | Loss: 43646.4219\n",
      "Ep  750 | Reward:  -574.69 | Eps: 0.105 | Loss: 353441.5625\n",
      "Ep  751 | Reward:  -571.42 | Eps: 0.104 | Loss: 32981.8906\n",
      "Ep  752 | Reward:  -646.46 | Eps: 0.104 | Loss: 6897.6606\n",
      "Ep  753 | Reward:  -863.68 | Eps: 0.104 | Loss: 8800.1699\n",
      "Ep  754 | Reward:  -740.86 | Eps: 0.103 | Loss: 7012.1904\n",
      "Ep  755 | Reward:  -634.70 | Eps: 0.103 | Loss: 23626.3867\n",
      "Ep  756 | Reward:  -717.60 | Eps: 0.103 | Loss: 11514.4688\n",
      "Ep  757 | Reward:  -726.30 | Eps: 0.103 | Loss: 26341.5605\n",
      "Ep  758 | Reward:  -719.89 | Eps: 0.102 | Loss: 9572.8506\n",
      "Ep  759 | Reward:  -656.52 | Eps: 0.102 | Loss: 10402.8408\n",
      "Ep  760 | Reward:  -736.31 | Eps: 0.102 | Loss: 6665.1963\n",
      "Ep  761 | Reward:  -398.13 | Eps: 0.101 | Loss: 9837.9395\n",
      "Ep  762 | Reward:  -564.33 | Eps: 0.101 | Loss: 16800.8203\n",
      "Ep  763 | Reward:  -661.97 | Eps: 0.101 | Loss: 15493.4150\n",
      "Ep  764 | Reward:  -939.96 | Eps: 0.100 | Loss: 22724.0840\n",
      "Ep  765 | Reward:  -521.87 | Eps: 0.100 | Loss: 8617.8320\n",
      "Ep  766 | Reward:  -647.89 | Eps: 0.100 | Loss: 7937.7451\n",
      "Ep  767 | Reward:  -671.98 | Eps: 0.100 | Loss: 5977.4404\n",
      "Ep  768 | Reward:  -305.80 | Eps: 0.099 | Loss: 5378.6548\n",
      "Ep  769 | Reward:  -631.70 | Eps: 0.099 | Loss: 7167.0312\n",
      "Ep  770 | Reward:  -361.55 | Eps: 0.099 | Loss: 7538.8735\n",
      "Ep  771 | Reward:   -85.47 | Eps: 0.098 | Loss: 6872.1748\n",
      "Ep  772 | Reward:  -847.42 | Eps: 0.098 | Loss: 7149.7026\n",
      "Ep  773 | Reward:  -636.38 | Eps: 0.098 | Loss: 8052.9204\n",
      "Ep  774 | Reward:  -763.93 | Eps: 0.097 | Loss: 6326.5508\n",
      "Ep  775 | Reward:  -136.16 | Eps: 0.097 | Loss: 7983.8574\n",
      "Ep  776 | Reward:  -156.50 | Eps: 0.097 | Loss: 9193.4502\n",
      "Ep  777 | Reward:  -783.70 | Eps: 0.097 | Loss: 9683.2236\n",
      "Ep  778 | Reward:  -695.81 | Eps: 0.096 | Loss: 6950.5449\n",
      "Ep  779 | Reward:  -141.37 | Eps: 0.096 | Loss: 14692.6084\n",
      "Ep  780 | Reward:  -655.91 | Eps: 0.096 | Loss: 5554.8496\n",
      "Ep  781 | Reward:  -162.88 | Eps: 0.095 | Loss: 7374.1719\n",
      "Ep  782 | Reward:  -232.89 | Eps: 0.095 | Loss: 4903.4839\n",
      "Ep  783 | Reward:  -687.32 | Eps: 0.095 | Loss: 10909.3154\n",
      "Ep  784 | Reward:  -103.44 | Eps: 0.095 | Loss: 10642.6836\n",
      "Ep  785 | Reward:  -735.34 | Eps: 0.094 | Loss: 5925.1787\n",
      "Ep  786 | Reward:  -433.97 | Eps: 0.094 | Loss: 3158.2634\n",
      "Ep  787 | Reward:  -494.65 | Eps: 0.094 | Loss: 12070.3467\n",
      "Ep  788 | Reward:  -331.47 | Eps: 0.093 | Loss: 11748.3320\n",
      "Ep  789 | Reward:  -739.19 | Eps: 0.093 | Loss: 16886.7285\n",
      "Ep  790 | Reward:  -542.19 | Eps: 0.093 | Loss: 12125.6289\n",
      "Ep  791 | Reward:  -484.70 | Eps: 0.093 | Loss: 8426.3301\n",
      "Ep  792 | Reward:  -376.20 | Eps: 0.092 | Loss: 5436.4351\n",
      "Ep  793 | Reward:  -364.05 | Eps: 0.092 | Loss: 6973.4775\n",
      "Ep  794 | Reward:  -668.93 | Eps: 0.092 | Loss: 3427.4778\n",
      "Ep  795 | Reward:  -230.14 | Eps: 0.091 | Loss: 4377.7554\n",
      "Ep  796 | Reward:  -104.79 | Eps: 0.091 | Loss: 5189.4873\n",
      "Ep  797 | Reward:  -409.81 | Eps: 0.091 | Loss: 5384.5371\n",
      "Ep  798 | Reward:  -243.19 | Eps: 0.091 | Loss: 11947.7627\n",
      "Ep  799 | Reward:  -437.05 | Eps: 0.090 | Loss: 7614.5752\n",
      "Ep  800 | Reward:  -441.93 | Eps: 0.090 | Loss: 6297.8896\n",
      "Ep  801 | Reward:  -368.21 | Eps: 0.090 | Loss: 10000.3633\n",
      "Ep  802 | Reward:  -614.21 | Eps: 0.090 | Loss: 35474.2734\n",
      "Ep  803 | Reward:  -865.58 | Eps: 0.089 | Loss: 2887.5537\n",
      "Ep  804 | Reward:  -849.97 | Eps: 0.089 | Loss: 15595.9824\n",
      "Ep  805 | Reward:  -375.92 | Eps: 0.089 | Loss: 18929.2559\n",
      "Ep  806 | Reward:  -888.54 | Eps: 0.089 | Loss: 4903.7188\n",
      "Ep  807 | Reward:  -723.78 | Eps: 0.088 | Loss: 8393.1660\n",
      "Ep  808 | Reward:  -298.86 | Eps: 0.088 | Loss: 3547.8557\n",
      "Ep  809 | Reward:  -479.02 | Eps: 0.088 | Loss: 9592.2090\n",
      "Ep  810 | Reward:  -524.29 | Eps: 0.087 | Loss: 6021.7988\n",
      "Ep  811 | Reward:  -261.29 | Eps: 0.087 | Loss: 10327.5820\n",
      "Ep  812 | Reward:  -555.03 | Eps: 0.087 | Loss: 7437.9009\n",
      "Ep  813 | Reward:  -565.24 | Eps: 0.087 | Loss: 7415.2163\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#añado yo\u001b[39;00m\n\u001b[32m      3\u001b[39m agent.save_model(\u001b[33m\"\u001b[39m\u001b[33mmodelo_DQN.h5\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\LunarEntorno\\DQN.py:239\u001b[39m, in \u001b[36mDQNAgent.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    236\u001b[39m total_reward += reward\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.memory) >= \u001b[38;5;28mself\u001b[39m.batch_size:\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupdate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    241\u001b[39m      loss = \u001b[32m0.0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\Desktop\\LunarEntorno\\DQN.py:176\u001b[39m, in \u001b[36mDQNAgent.update_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    174\u001b[39m next_q_values = \u001b[38;5;28mself\u001b[39m.target_network(next_states).max(\u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m].detach()\n\u001b[32m    175\u001b[39m \u001b[38;5;66;03m# 5. Compute the expected Q-values\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m expected_q_values = rewards + \u001b[38;5;28mself\u001b[39m.gamma * next_q_values * (\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m)\n\u001b[32m    178\u001b[39m loss = F.mse_loss(q_values, expected_q_values)\n\u001b[32m    179\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:33\u001b[39m, in \u001b[36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m(f):\n\u001b[32m     31\u001b[39m     assigned = functools.WRAPPER_ASSIGNMENTS\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[38;5;129m@functools\u001b[39m.wraps(f, assigned=assigned)\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(*args, **kwargs):\n\u001b[32m     35\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     36\u001b[39m             \u001b[38;5;66;03m# See https://github.com/pytorch/pytorch/issues/75462\u001b[39;00m\n\u001b[32m     37\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "agent.train()\n",
    "#añado yo\n",
    "agent.save_model(\"modelo_DQN.h6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNetwork:\n",
      " DQN(\n",
      "  (fc1): Linear(in_features=8, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (out): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# agent with epsilon = 0.0 (no exploration)\n",
    "agent = DQNAgent(lunar, epsilon=0.0)\n",
    "agent.load_model(\"modelo_DQN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_lunar_lander' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtest_lunar_lander\u001b[49m(steps_to_run_before_pause=\u001b[32m25\u001b[39m, agent=agent, episodes=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'test_lunar_lander' is not defined"
     ]
    }
   ],
   "source": [
    "test_lunar_lander(steps_to_run_before_pause=25, agent=agent, episodes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from REINFORCE import REINFORCEAgent\n",
    "lunar = LunarLanderEnv(render_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = REINFORCEAgent(lunar, episodes=5000)\n",
    "# agent.load_model(\"modelo_REINFORCE.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = REINFORCEAgent(lunar)\n",
    "agent.load_model(\"modelo_REINFORCE.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lunar_lander(steps_to_run_before_pause=75, agent=agent, episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
